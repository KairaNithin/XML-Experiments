{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLM Freezed Conv Unfreezed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6cf00bc89dea4838a8d61d00fe1f3dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87454a2bf8784f0f8778588ca495495e",
              "IPY_MODEL_40c70f2b723e41f7a5b1069cf77937d0"
            ],
            "layout": "IPY_MODEL_38e0d24a8cf74be59a0c66293e98e307"
          }
        },
        "87454a2bf8784f0f8778588ca495495e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1db1597778d44d8f980e7f4722355702",
            "max": 541,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86d0bdaa039b4ae3ba31800c0e322b10",
            "value": 541
          }
        },
        "40c70f2b723e41f7a5b1069cf77937d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0559baf2d8844ba39bd2a9f3738b00da",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6e60ee394c5d40319f3e1a4f6cecd8c4",
            "value": " 541/541 [00:31&lt;00:00, 17.3B/s]"
          }
        },
        "38e0d24a8cf74be59a0c66293e98e307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1db1597778d44d8f980e7f4722355702": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86d0bdaa039b4ae3ba31800c0e322b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0559baf2d8844ba39bd2a9f3738b00da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e60ee394c5d40319f3e1a4f6cecd8c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adf79b769b984963aaefd62ff6947e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85078aabe8014d72aafa1398963c880a",
              "IPY_MODEL_aaf5e194359142e6a88c46f830346b64"
            ],
            "layout": "IPY_MODEL_715de49997b34fc4b32e7e03b75df6a9"
          }
        },
        "85078aabe8014d72aafa1398963c880a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59aa2a7926e54df59edfb9a01621b18c",
            "max": 1112256686,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99ab377622ce4811a89563ff8dac69d6",
            "value": 1112256686
          }
        },
        "aaf5e194359142e6a88c46f830346b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2988257d774f4e888abff75761555545",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9075d6edeb644553a598cc68c89310b0",
            "value": " 1.11G/1.11G [00:30&lt;00:00, 37.0MB/s]"
          }
        },
        "715de49997b34fc4b32e7e03b75df6a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59aa2a7926e54df59edfb9a01621b18c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ab377622ce4811a89563ff8dac69d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2988257d774f4e888abff75761555545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9075d6edeb644553a598cc68c89310b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30a25c1567404738ae1ea967183f2f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0016a2fab7e14c959b1cda0108c2916a",
              "IPY_MODEL_efa4cfb566ea4948a403a78e712869d3"
            ],
            "layout": "IPY_MODEL_f94112492bf24936a23597050761f462"
          }
        },
        "0016a2fab7e14c959b1cda0108c2916a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1885716c90394985b6f86ccb61510ecf",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_773d793fef6a46778fed40536c37a64b",
            "value": 5069051
          }
        },
        "efa4cfb566ea4948a403a78e712869d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_602d321380ee4128b1bf01382b030a28",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0311e2ea8aa548c58f0be83838b74fc8",
            "value": " 5.07M/5.07M [00:02&lt;00:00, 2.01MB/s]"
          }
        },
        "f94112492bf24936a23597050761f462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1885716c90394985b6f86ccb61510ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773d793fef6a46778fed40536c37a64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "602d321380ee4128b1bf01382b030a28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0311e2ea8aa548c58f0be83838b74fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da12028fd51441ef854434255bd63690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1cebf94428a47b08d91eb3d376cb864",
              "IPY_MODEL_fe877a8afec641df82e69caecd3faad9"
            ],
            "layout": "IPY_MODEL_0725ac84c0fe4db4a5b8ee5e5f15788b"
          }
        },
        "d1cebf94428a47b08d91eb3d376cb864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2842421fa244fcd9385ccff1466996d",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b26de9c295ed4a4b893bd2d799793e7e",
            "value": 150
          }
        },
        "fe877a8afec641df82e69caecd3faad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_844af92f3dd8438aac91ac8c76253909",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5813c1af2aba47359a640a34c10ba246",
            "value": " 150/150 [00:00&lt;00:00, 332B/s]"
          }
        },
        "0725ac84c0fe4db4a5b8ee5e5f15788b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2842421fa244fcd9385ccff1466996d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b26de9c295ed4a4b893bd2d799793e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "844af92f3dd8438aac91ac8c76253909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5813c1af2aba47359a640a34c10ba246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed07bff31bde4d148b8d8bcf4734b9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_479c5e05879a4821ae1b5b1f00033819",
              "IPY_MODEL_14e98f59bdf447ccb4c8f2dca5464e86"
            ],
            "layout": "IPY_MODEL_942d34c4a6ab4e509ae903db2d7857e9"
          }
        },
        "479c5e05879a4821ae1b5b1f00033819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca2dac121a114882b20828bd63e956f5",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f0484de057d4a52bb9c15d6a52263fc",
            "value": 147
          }
        },
        "14e98f59bdf447ccb4c8f2dca5464e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c008421054a54fa5a4733df127dbf34b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9ee0666b9c41422f94e16f09b2b2d729",
            "value": " 147/147 [00:00&lt;00:00, 210B/s]"
          }
        },
        "942d34c4a6ab4e509ae903db2d7857e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca2dac121a114882b20828bd63e956f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f0484de057d4a52bb9c15d6a52263fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "c008421054a54fa5a4733df127dbf34b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ee0666b9c41422f94e16f09b2b2d729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7ce9b96da854bb9893ec8fa1cf6033e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d23d373f144247bb9e0f06050f1869d3",
              "IPY_MODEL_317c1e42df3541cfacfabd601995fe9f"
            ],
            "layout": "IPY_MODEL_b32926a9b7d14e048056751b4550737f"
          }
        },
        "d23d373f144247bb9e0f06050f1869d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9c3cdd6706841d78fb508ccb6e58376",
            "max": 1014108634,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d90fa9dc19cc4f1c84adb8cc2560b6fa",
            "value": 1014108634
          }
        },
        "317c1e42df3541cfacfabd601995fe9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e01d7e6e934c42e8bab30a67cad61617",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_64d84831efd74489b7493a34bafd62fc",
            "value": " 1.01G/1.01G [01:03&lt;00:00, 16.0MB/s]"
          }
        },
        "b32926a9b7d14e048056751b4550737f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9c3cdd6706841d78fb508ccb6e58376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d90fa9dc19cc4f1c84adb8cc2560b6fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "e01d7e6e934c42e8bab30a67cad61617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64d84831efd74489b7493a34bafd62fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KairaNithin/XML-Experiments/blob/main/XLM_Freezed_Conv_Unfreezed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6aVh5DZ8m-H",
        "outputId": "8b62010b-3f72-4a3a-ad77-bdbbd1994392"
      },
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# !git clone https://github.com/sayarghoshroy/Hate-Speech-Detection.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/9d/abacb6f7bb63df39285c55bb51b6403a7fd93ac2aea48b01f6215175446c/sentence-transformers-1.1.1.tar.gz (81kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 3.0MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.3MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 32.3MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.1-cp37-none-any.whl size=123338 sha256=bb5780927183df45f2d94093e8ea3ca0f2be2e228cb5d070ec1a4fdc331e40e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/89/29/45e45adc162b50f97f71801e8b07947c9cfe2b3ae7dbf37896\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 sentence-transformers-1.1.1 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.6.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n60UPiTv9Cn3"
      },
      "source": [
        "import random\n",
        "import pickle\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "# Check where we need this\n",
        "# from nltk.corpus import stopwordsm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "import gensim.models as gsm\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "from tqdm import tqdm \n",
        "import gc\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phdtywu9Ajnr",
        "outputId": "367598c4-d692-4644-9a99-2fc56c49c061"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxQXM09C9LNS",
        "outputId": "74f5739e-1f70-4b1c-b10e-2359f95d336d"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16v_lom4GN7P"
      },
      "source": [
        "train_dataloader, valid_dataloader, dataloader, dataloaders = torch.load('drive/MyDrive/train_dataloader.pth'), torch.load('drive/MyDrive/valid_dataloader.pth'),torch.load('drive/MyDrive/dataloader.pth'),torch.load('drive/MyDrive/dataloaders.pth')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4q_z34yb67Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu8RpkTsGuF3"
      },
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "  \"\"\" Classification head for the Roberta Model \"\"\" \n",
        "  def __init__(self, numberOfClasses, hidden_size_bert, hidden_size_post_feats, dropout_val = 0.2):\n",
        "    super().__init__()\n",
        "   \n",
        "    self.denseInit = nn.Linear(hidden_size_post_feats, hidden_size_bert)\n",
        "    self.dense = nn.Linear(hidden_size_bert, hidden_size_bert)\n",
        "    self.dropout = nn.Dropout(dropout_val)\n",
        "    self.output = nn.Linear(hidden_size_bert, numberOfClasses)\n",
        "    \n",
        "  def forward(self, x):\n",
        "\n",
        "   \n",
        "    x = self.dropout(x)\n",
        "    x = self.denseInit(x)\n",
        "    x = torch.tanh(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense(x)\n",
        "    x  = torch.tanh(x)\n",
        "    x = self.dropout(x)\n",
        "    x  = self.output(x)\n",
        "    return x\n",
        "\n",
        "# def attn(x,model_dim,emoji,hashtag):\n",
        "\n",
        "#         mlp = nn.Sequential(\n",
        "\n",
        "#             nn.Linear(model_dim, model_dim // 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(model_dim // 2, model_dim),\n",
        "#             nn.ReLU()\n",
        "#             )\n",
        "#         normal_inputs = x.permute(0,2,1)\n",
        "        \n",
        "#         max = F.max_pool1d(normal_inputs,kernel_size=normal_inputs.shape[2]).squeeze(2)\n",
        "\n",
        "#         avg = F.avg_pool1d(normal_inputs,kernel_size=normal_inputs.shape[2]).squeeze(2)\n",
        "\n",
        "#         max_n = mlp(max)\n",
        "\n",
        "#         avg_n = mlp(avg)\n",
        "\n",
        "#         res_channel = torch.cat((max_n.unsqueeze(1),avg_n.unsqueeze(1)),axis=1)\n",
        "\n",
        "#         # spacial attention\n",
        "\n",
        "\n",
        "#         res_spacial = nn.Conv1d(2,1,kernel_size=1)(torch.cat((max.unsqueeze(1),avg.unsqueeze(1)),axis=1))\n",
        "\n",
        "#         res_attention = torch.cat((res_channel,res_spacial,hashtag.unsqueeze(1)),axis=1)\n",
        "\n",
        "#         # print(res_attention.shape)\n",
        "\n",
        "#         # gated convolutional networks\n",
        "\n",
        "#         A = nn.Conv1d(4, 8, kernel_size=(469))(res_attention)\n",
        "#         b = nn.Parameter()\n",
        "#         B = nn.Conv1d(4, 8, kernel_size=(469))(res_attention)\n",
        "\n",
        "\n",
        "#         h = A*torch.sigmoid(B)\n",
        "\n",
        "#         # print(h.shape)\n",
        "#         ans = torch.cat((h,emoji.unsqueeze(1)),axis=1)\n",
        "\n",
        "\n",
        "#         ans = nn.Conv1d(9,1,kernel_size=1)(ans)\n",
        "\n",
        "#         # ans = ans.view(ans.shape[0],-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         # ans = nn.Linear(ans.shape[1],900)(ans.type(torch.float))\n",
        "\n",
        "#         ans = nn.Linear(300,2)(ans)\n",
        "\n",
        "#         # ans = nn.Linear(ans.shape[1]*ans.shape[2],(ans.shape[1]*ans.shape[2]/2))(ans)\n",
        "\n",
        "#         return ans\n",
        "\n",
        "# class TextClassification(nn.Module):\n",
        "#   \"\"\" Classifier with feature injection \"\"\"\n",
        "#   def __init__(self, numberOfClasses,dropout_val = 0.1, batch_size = 16):\n",
        "#      super(TextClassification, self).__init__()\n",
        "#      self.bert = XLMRobertaModel.from_pretrained(\"sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens\")\n",
        "#      self.classifier = ClassificationHead(numberOfClasses, self.bert.config.hidden_size, (self.bert.config.hidden_size * 2 + 300) , dropout_val)\n",
        "#      self.mlp = nn.Sequential(\n",
        "\n",
        "#             nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size // 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(self.bert.config.hidden_size // 2, self.bert.config.hidden_size),\n",
        "#             nn.ReLU()\n",
        "#             )\n",
        "#   def forward(self, input_seq, attention_mask, emoji, hashTag):\n",
        "#     # print(\"input seq :\",input_seq.shape)\n",
        "#     bert_pooled_output = self.bert(input_seq, attention_mask=attention_mask)[0]\n",
        "#     bert_pooled_output = bert_pooled_output.permute(0,2,1)\n",
        "\n",
        "#     max = F.max_pool1d(bert_pooled_output,kernel_size=bert_pooled_output.shape[2]).squeeze(2)\n",
        "\n",
        "#     avg = F.avg_pool1d(bert_pooled_output,kernel_size=bert_pooled_output.shape[2]).squeeze(2)\n",
        "\n",
        "#     max_n = self.mlp(max)\n",
        "\n",
        "#     avg_n = self.mlp(avg)\n",
        "\n",
        "#     res_channel = torch.cat((max_n.unsqueeze(1),avg_n.unsqueeze(1)),axis=1)\n",
        "\n",
        "#     bert_pooled_output = nn.Conv1d(2,1,kernel_size=1)(res_channel)\n",
        "\n",
        "#     bert_pooled_output = bert_pooled_output.squeeze(1)\n",
        "    \n",
        "    \n",
        "#     print(\"bert_pooled_output shape :\",bert_pooled_output.shape)\n",
        "#     bert_pooled_out_feat = torch.cat([bert_pooled_output, emoji, hashTag], axis = 1)\n",
        "\n",
        "#     # bert_pooled_out_feat = attn(bert_pooled_output,768)\n",
        "#     output = self.classifier(bert_pooled_out_feat)\n",
        "\n",
        "#     # output = attn(bert_pooled_output,768,emoji,hashTag)\n",
        "#     return output\n",
        "\n",
        "class TextClassification(nn.Module):\n",
        "  \"\"\" Classifier with feature injection \"\"\"\n",
        "  def __init__(self, numberOfClasses,dropout_val = 0.1, batch_size = 16):\n",
        "     super(TextClassification, self).__init__()\n",
        "     self.bert = XLMRobertaModel.from_pretrained(\"sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens\")\n",
        "     self.classifier = ClassificationHead(numberOfClasses, self.bert.config.hidden_size, (self.bert.config.hidden_size * 2 + 300) , dropout_val)\n",
        "     self.mlp = nn.Sequential(\n",
        "\n",
        "            nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.bert.config.hidden_size // 2, self.bert.config.hidden_size),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "     self.conv0 = nn.Conv1d(2,1,kernel_size=1)\n",
        "  def forward(self, input_seq, attention_mask, emoji, hashTag):\n",
        "    # print(\"input seq :\",input_seq.shape)\n",
        "    bert_pooled_output = self.bert(input_seq, attention_mask=attention_mask)[0]\n",
        "\n",
        "    bert_pooled_output = bert_pooled_output.permute(0,2,1)\n",
        "\n",
        "    # max = F.max_pool1d(bert_pooled_output,kernel_size=bert_pooled_output.shape[2]).squeeze(2)\n",
        "\n",
        "    avg = F.avg_pool1d(bert_pooled_output,kernel_size=bert_pooled_output.shape[2]).squeeze(2)\n",
        "    max = F.max_pool1d(bert_pooled_output,kernel_size=bert_pooled_output.shape[2]).squeeze(2)\n",
        "\n",
        "    # print(\"max shape :\",max.shape)\n",
        "\n",
        "    avg = self.mlp(avg)\n",
        "    max = self.mlp(max)\n",
        "\n",
        "    # print(\"max shape :\",max.shape)\n",
        "\n",
        "    final = self.conv0(torch.cat((avg.unsqueeze(1),max.unsqueeze(1)),axis=1))\n",
        "    # max_n = self.mlp(max)\n",
        "\n",
        "    # avg_n = self.mlp(avg)\n",
        "\n",
        "    # res_channel = torch.cat((max_n.unsqueeze(1),avg_n.unsqueeze(1)),axis=1)\n",
        "\n",
        "    # bert_pooled_output = nn.Conv1d(2,1,kernel_size=1)(res_channel)\n",
        "\n",
        "    # bert_pooled_output = bert_pooled_output.squeeze(1)\n",
        "    \n",
        "    \n",
        "    # print(\"bert_pooled_output shape :\",final.shape)\n",
        "    bert_pooled_out_feat = torch.cat([final.squeeze(1), emoji, hashTag], axis = 1)\n",
        "\n",
        "    # bert_pooled_out_feat = attn(bert_pooled_output,768)\n",
        "    output = self.classifier(bert_pooled_out_feat)\n",
        "\n",
        "    # output = attn(bert_pooled_output,768,emoji,hashTag)\n",
        "    return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XuhgR3WGu5E"
      },
      "source": [
        "model_name = 'adaptive'\n",
        "model_loc = 'hasoc_saved/'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gMVxOh4NUo6"
      },
      "source": [
        "def modelEvaluate(model, valid_dataloader = valid_dataloader, task = 1):\n",
        "  gc.collect()\n",
        "  if task == 1:\n",
        "    taskIndex = 6\n",
        "  elif task == 2:\n",
        "    taskIndex = 7\n",
        "  model.eval()\n",
        "  predictions, true_labels = [], []\n",
        "  logits = []\n",
        "  # Predict \n",
        "  for batch in valid_dataloader:\n",
        "    # Add batch to GPU\n",
        "    b_input_ids = batch[1]\n",
        "    b_input_mask = batch[2]\n",
        "    b_labels = batch[taskIndex]\n",
        "    b_emoji = batch[5]\n",
        "    b_hashtag = batch[4]\n",
        "    with torch.no_grad():\n",
        "      pred = model(b_input_ids,b_input_mask ,b_emoji.float(), b_hashtag.float())\n",
        "    logits.append(pred.detach().cpu().numpy())\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    flat_true_labels = np.concatenate(true_labels, axis = 0)\n",
        "    predictions = []\n",
        "    for i in logits:\n",
        "      for j in i:\n",
        "        predictions.append(j)\n",
        "    flat_predictions = [np.argmax(i) for i in predictions]\n",
        "    assert(len(flat_predictions) == len(flat_true_labels))\n",
        "    return flat_predictions, flat_true_labels"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3UyFk-8NZVd"
      },
      "source": [
        "path = \"/content/Hate-Speech-Detection\" + model_name + \".pt\"\n",
        "scale = 1"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuNB0xtMNfQ9"
      },
      "source": [
        "def make_optim(model, rate = 2e-5):\n",
        "  return AdamW(model.parameters(),\n",
        "                lr = rate, # default = 5e-5, using 2e-5\n",
        "                eps = 1e-8) # default = 1e-8\n",
        "\n",
        "def train_model(train_dataloader, valid_dataloader, numberOfEpochs = 10, task = 1):\n",
        "  \"\"\" Train Loop for the model \"\"\"\n",
        "  scale = 1\n",
        "  if task == 2:\n",
        "    classNum = 4\n",
        "    taskIndex = 7\n",
        "  elif task == 1:\n",
        "    classNum = 2\n",
        "    taskIndex = 6\n",
        "  else:\n",
        "    raise NameError(\"Task not defined\")\n",
        "  total_steps = len(train_dataloader)\n",
        "  print(\"Start\")\n",
        "\n",
        "  model = TextClassification(classNum) # task 1 \n",
        "\n",
        "   \n",
        "  for params in model.bert.parameters():\n",
        "    params.requires_grad = False\n",
        "  for params in model.classifier.parameters():\n",
        "    params.requires_grad = False\n",
        "  print(\"Parameters to be trained :\",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "  if device == \"gpu\":\n",
        "    model.cuda()\n",
        "\n",
        "  \n",
        "  loss_function = nn.CrossEntropyLoss().to(device)\n",
        "  epoch_loss = 0\n",
        "  batch_accuracy_scores = []\n",
        "  global_pred = []\n",
        "  global_label = []\n",
        "\n",
        "  present_rate = 2e-5\n",
        "  old_best = -1\n",
        "  epoch = 0\n",
        "\n",
        "  while(1):\n",
        "    # when the learn rate falls below a lower threshold, you stop your training\n",
        "    # until that moment, march on\n",
        "    epoch += 1\n",
        "    print(\"\\nEpoch:\", epoch)\n",
        "    print(\"Present Rate: \" + str(present_rate))\n",
        "    optimizer = make_optim(model, present_rate)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "    gc.collect()\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batch_accuracy_scores = []\n",
        "    train_data_count = float(len(train_dataloader))\n",
        "\n",
        "    # to check if performance with default weights\n",
        "    predictions, true_labels = modelEvaluate(model, valid_dataloader, task)\n",
        "    score_now = f1_score(true_labels, predictions, average = 'macro')\n",
        "    print(\"Validation Macro: \" + str(score_now))\n",
        "\n",
        "    if (score_now > old_best):\n",
        "      print(\"Continuing on track\")\n",
        "      old_best = score_now\n",
        "\n",
        "      # delete previous best \n",
        "      delete_filename = path\n",
        "      open(delete_filename, 'w').close() # overwrite and make the file blank instead\n",
        "      os.remove(delete_filename) # delete the blank file from google drive will move the file to bin instead\n",
        "      torch.save(model.state_dict(), path)\n",
        "\n",
        "    else:\n",
        "      print(\"Backtrack\")\n",
        "      model.load_state_dict(torch.load(path))\n",
        "      present_rate /= (4 * scale)\n",
        "      scale *= 4\n",
        "      if present_rate < 1e-8:\n",
        "        break\n",
        "\n",
        "    # For quick eval\n",
        "    cnt = 0\n",
        "    # for i, batch in tqdm(enumerate(train_dataloader)):\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        print(\"Iter: \" + str(cnt + 1))\n",
        "        # COMMENT OUT THE NEXT 2 LINES IN ACTUAL TRAINING\n",
        "        # if cnt == 4:\n",
        "        #   break\n",
        "        cnt += 1\n",
        "        b_input_ids = batch[1]\n",
        "        b_input_mask = batch[2]\n",
        "        b_labels = batch[taskIndex]\n",
        "        b_emoji = batch[5]\n",
        "        b_hashtag = batch[4]\n",
        "        pred = model(b_input_ids,b_input_mask ,b_emoji.float(), b_hashtag.float())\n",
        "        loss = loss_function(pred.view(-1, classNum), b_labels.view(-1))\n",
        "        with torch.no_grad():\n",
        "          epoch_loss += (loss.item() * len(b_labels))\n",
        "          global_pred.append(pred)\n",
        "          global_label.append(b_labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3erqakTNvyU",
        "outputId": "e3105514-2d96-48da-edb7-5393e6cd332d"
      },
      "source": [
        "gc.collect()\n",
        "model = train_model(train_dataloader, valid_dataloader, 2, task = 1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "Parameters to be trained : 590979\n",
            "\n",
            "Epoch: 1\n",
            "Present Rate: 2e-05\n",
            "Validation Macro: 0.15789473684210525\n",
            "Continuing on track\n",
            "Iter: 1\n",
            "Iter: 2\n",
            "Iter: 3\n",
            "Iter: 4\n",
            "Iter: 5\n",
            "Iter: 6\n",
            "Iter: 7\n",
            "Iter: 8\n",
            "Iter: 9\n",
            "Iter: 10\n",
            "Iter: 11\n",
            "Iter: 12\n",
            "Iter: 13\n",
            "Iter: 14\n",
            "Iter: 15\n",
            "Iter: 16\n",
            "Iter: 17\n",
            "Iter: 18\n",
            "Iter: 19\n",
            "Iter: 20\n",
            "Iter: 21\n",
            "Iter: 22\n",
            "Iter: 23\n",
            "Iter: 24\n",
            "Iter: 25\n",
            "Iter: 26\n",
            "Iter: 27\n",
            "Iter: 28\n",
            "Iter: 29\n",
            "Iter: 30\n",
            "Iter: 31\n",
            "Iter: 32\n",
            "Iter: 33\n",
            "Iter: 34\n",
            "Iter: 35\n",
            "Iter: 36\n",
            "Iter: 37\n",
            "Iter: 38\n",
            "Iter: 39\n",
            "Iter: 40\n",
            "Iter: 41\n",
            "Iter: 42\n",
            "Iter: 43\n",
            "Iter: 44\n",
            "Iter: 45\n",
            "Iter: 46\n",
            "Iter: 47\n",
            "Iter: 48\n",
            "Iter: 49\n",
            "Iter: 50\n",
            "Iter: 51\n",
            "Iter: 52\n",
            "Iter: 53\n",
            "Iter: 54\n",
            "Iter: 55\n",
            "Iter: 56\n",
            "Iter: 57\n",
            "Iter: 58\n",
            "Iter: 59\n",
            "Iter: 60\n",
            "Iter: 61\n",
            "Iter: 62\n",
            "Iter: 63\n",
            "Iter: 64\n",
            "Iter: 65\n",
            "Iter: 66\n",
            "Iter: 67\n",
            "Iter: 68\n",
            "Iter: 69\n",
            "Iter: 70\n",
            "Iter: 71\n",
            "Iter: 72\n",
            "Iter: 73\n",
            "Iter: 74\n",
            "Iter: 75\n",
            "Iter: 76\n",
            "Iter: 77\n",
            "Iter: 78\n",
            "Iter: 79\n",
            "Iter: 80\n",
            "Iter: 81\n",
            "Iter: 82\n",
            "Iter: 83\n",
            "Iter: 84\n",
            "Iter: 85\n",
            "Iter: 86\n",
            "Iter: 87\n",
            "Iter: 88\n",
            "Iter: 89\n",
            "Iter: 90\n",
            "Iter: 91\n",
            "Iter: 92\n",
            "Iter: 93\n",
            "Iter: 94\n",
            "Iter: 95\n",
            "Iter: 96\n",
            "Iter: 97\n",
            "Iter: 98\n",
            "Iter: 99\n",
            "Iter: 100\n",
            "Iter: 101\n",
            "Iter: 102\n",
            "Iter: 103\n",
            "Iter: 104\n",
            "Iter: 105\n",
            "Iter: 106\n",
            "Iter: 107\n",
            "Iter: 108\n",
            "Iter: 109\n",
            "Iter: 110\n",
            "Iter: 111\n",
            "Iter: 112\n",
            "Iter: 113\n",
            "Iter: 114\n",
            "Iter: 115\n",
            "Iter: 116\n",
            "Iter: 117\n",
            "Iter: 118\n",
            "Iter: 119\n",
            "Iter: 120\n",
            "Iter: 121\n",
            "Iter: 122\n",
            "Iter: 123\n",
            "Iter: 124\n",
            "Iter: 125\n",
            "Iter: 126\n",
            "Iter: 127\n",
            "Iter: 128\n",
            "Iter: 129\n",
            "Iter: 130\n",
            "Iter: 131\n",
            "Iter: 132\n",
            "Iter: 133\n",
            "Iter: 134\n",
            "Iter: 135\n",
            "Iter: 136\n",
            "Iter: 137\n",
            "Iter: 138\n",
            "Iter: 139\n",
            "Iter: 140\n",
            "Iter: 141\n",
            "Iter: 142\n",
            "Iter: 143\n",
            "Iter: 144\n",
            "Iter: 145\n",
            "Iter: 146\n",
            "Iter: 147\n",
            "Iter: 148\n",
            "Iter: 149\n",
            "Iter: 150\n",
            "Iter: 151\n",
            "Iter: 152\n",
            "Iter: 153\n",
            "Iter: 154\n",
            "Iter: 155\n",
            "Iter: 156\n",
            "Iter: 157\n",
            "Iter: 158\n",
            "Iter: 159\n",
            "Iter: 160\n",
            "Iter: 161\n",
            "Iter: 162\n",
            "Iter: 163\n",
            "Iter: 164\n",
            "Iter: 165\n",
            "Iter: 166\n",
            "Iter: 167\n",
            "Iter: 168\n",
            "Iter: 169\n",
            "Iter: 170\n",
            "Iter: 171\n",
            "Iter: 172\n",
            "Iter: 173\n",
            "Iter: 174\n",
            "Iter: 175\n",
            "Iter: 176\n",
            "Iter: 177\n",
            "Iter: 178\n",
            "Iter: 179\n",
            "Iter: 180\n",
            "Iter: 181\n",
            "Iter: 182\n",
            "Iter: 183\n",
            "Iter: 184\n",
            "Iter: 185\n",
            "Iter: 186\n",
            "Iter: 187\n",
            "Iter: 188\n",
            "Iter: 189\n",
            "Iter: 190\n",
            "Iter: 191\n",
            "Iter: 192\n",
            "Iter: 193\n",
            "Iter: 194\n",
            "Iter: 195\n",
            "Iter: 196\n",
            "Iter: 197\n",
            "Iter: 198\n",
            "Iter: 199\n",
            "Iter: 200\n",
            "Iter: 201\n",
            "Iter: 202\n",
            "Iter: 203\n",
            "Iter: 204\n",
            "Iter: 205\n",
            "Iter: 206\n",
            "Iter: 207\n",
            "Iter: 208\n",
            "Iter: 209\n",
            "Iter: 210\n",
            "Iter: 211\n",
            "Iter: 212\n",
            "Iter: 213\n",
            "Iter: 214\n",
            "Iter: 215\n",
            "Iter: 216\n",
            "Iter: 217\n",
            "Iter: 218\n",
            "Iter: 219\n",
            "Iter: 220\n",
            "Iter: 221\n",
            "Iter: 222\n",
            "Iter: 223\n",
            "Iter: 224\n",
            "Iter: 225\n",
            "Iter: 226\n",
            "Iter: 227\n",
            "Iter: 228\n",
            "Iter: 229\n",
            "Iter: 230\n",
            "Iter: 231\n",
            "Iter: 232\n",
            "Iter: 233\n",
            "Iter: 234\n",
            "Iter: 235\n",
            "Iter: 236\n",
            "Iter: 237\n",
            "Iter: 238\n",
            "Iter: 239\n",
            "Iter: 240\n",
            "Iter: 241\n",
            "Iter: 242\n",
            "Iter: 243\n",
            "Iter: 244\n",
            "Iter: 245\n",
            "Iter: 246\n",
            "Iter: 247\n",
            "Iter: 248\n",
            "Iter: 249\n",
            "Iter: 250\n",
            "Iter: 251\n",
            "Iter: 252\n",
            "Iter: 253\n",
            "Iter: 254\n",
            "Iter: 255\n",
            "Iter: 256\n",
            "Iter: 257\n",
            "Iter: 258\n",
            "Iter: 259\n",
            "Iter: 260\n",
            "Iter: 261\n",
            "Iter: 262\n",
            "Iter: 263\n",
            "Iter: 264\n",
            "Iter: 265\n",
            "Iter: 266\n",
            "Iter: 267\n",
            "Iter: 268\n",
            "Iter: 269\n",
            "Iter: 270\n",
            "Iter: 271\n",
            "Iter: 272\n",
            "Iter: 273\n",
            "Iter: 274\n",
            "Iter: 275\n",
            "Iter: 276\n",
            "Iter: 277\n",
            "Iter: 278\n",
            "Iter: 279\n",
            "Iter: 280\n",
            "Iter: 281\n",
            "Iter: 282\n",
            "Iter: 283\n",
            "Iter: 284\n",
            "Iter: 285\n",
            "Iter: 286\n",
            "Iter: 287\n",
            "Iter: 288\n",
            "Iter: 289\n",
            "Iter: 290\n",
            "Iter: 291\n",
            "Iter: 292\n",
            "Iter: 293\n",
            "Iter: 294\n",
            "Iter: 295\n",
            "Iter: 296\n",
            "Iter: 297\n",
            "Iter: 298\n",
            "Iter: 299\n",
            "Iter: 300\n",
            "Iter: 301\n",
            "Iter: 302\n",
            "Iter: 303\n",
            "Iter: 304\n",
            "Iter: 305\n",
            "Iter: 306\n",
            "Iter: 307\n",
            "Iter: 308\n",
            "Iter: 309\n",
            "Iter: 310\n",
            "Iter: 311\n",
            "Iter: 312\n",
            "Iter: 313\n",
            "Iter: 314\n",
            "Iter: 315\n",
            "Iter: 316\n",
            "Iter: 317\n",
            "Iter: 318\n",
            "Iter: 319\n",
            "Iter: 320\n",
            "Iter: 321\n",
            "Iter: 322\n",
            "Iter: 323\n",
            "Iter: 324\n",
            "Iter: 325\n",
            "Iter: 326\n",
            "Iter: 327\n",
            "Iter: 328\n",
            "Iter: 329\n",
            "Iter: 330\n",
            "Iter: 331\n",
            "Iter: 332\n",
            "Iter: 333\n",
            "Iter: 334\n",
            "Iter: 335\n",
            "Iter: 336\n",
            "Iter: 337\n",
            "Iter: 338\n",
            "Iter: 339\n",
            "Iter: 340\n",
            "Iter: 341\n",
            "Iter: 342\n",
            "Iter: 343\n",
            "Iter: 344\n",
            "Iter: 345\n",
            "Iter: 346\n",
            "Iter: 347\n",
            "Iter: 348\n",
            "Iter: 349\n",
            "Iter: 350\n",
            "Iter: 351\n",
            "Iter: 352\n",
            "Iter: 353\n",
            "Iter: 354\n",
            "Iter: 355\n",
            "Iter: 356\n",
            "Iter: 357\n",
            "Iter: 358\n",
            "Iter: 359\n",
            "Iter: 360\n",
            "Iter: 361\n",
            "Iter: 362\n",
            "Iter: 363\n",
            "Iter: 364\n",
            "Iter: 365\n",
            "Iter: 366\n",
            "Iter: 367\n",
            "Iter: 368\n",
            "Iter: 369\n",
            "Iter: 370\n",
            "Iter: 371\n",
            "Iter: 372\n",
            "Iter: 373\n",
            "Iter: 374\n",
            "Iter: 375\n",
            "Iter: 376\n",
            "Iter: 377\n",
            "Iter: 378\n",
            "Iter: 379\n",
            "Iter: 380\n",
            "Iter: 381\n",
            "Iter: 382\n",
            "Iter: 383\n",
            "Iter: 384\n",
            "Iter: 385\n",
            "Iter: 386\n",
            "Iter: 387\n",
            "Iter: 388\n",
            "Iter: 389\n",
            "Iter: 390\n",
            "Iter: 391\n",
            "Iter: 392\n",
            "Iter: 393\n",
            "Iter: 394\n",
            "Iter: 395\n",
            "Iter: 396\n",
            "Iter: 397\n",
            "Iter: 398\n",
            "Iter: 399\n",
            "Iter: 400\n",
            "Iter: 401\n",
            "Iter: 402\n",
            "Iter: 403\n",
            "Iter: 404\n",
            "Iter: 405\n",
            "Iter: 406\n",
            "Iter: 407\n",
            "Iter: 408\n",
            "Iter: 409\n",
            "Iter: 410\n",
            "Iter: 411\n",
            "Iter: 412\n",
            "Iter: 413\n",
            "Iter: 414\n",
            "Iter: 415\n",
            "Iter: 416\n",
            "Iter: 417\n",
            "Iter: 418\n",
            "Iter: 419\n",
            "Iter: 420\n",
            "Iter: 421\n",
            "Iter: 422\n",
            "Iter: 423\n",
            "Iter: 424\n",
            "Iter: 425\n",
            "Iter: 426\n",
            "Iter: 427\n",
            "Iter: 428\n",
            "Iter: 429\n",
            "Iter: 430\n",
            "Iter: 431\n",
            "Iter: 432\n",
            "Iter: 433\n",
            "Iter: 434\n",
            "Iter: 435\n",
            "Iter: 436\n",
            "Iter: 437\n",
            "Iter: 438\n",
            "Iter: 439\n",
            "Iter: 440\n",
            "Iter: 441\n",
            "Iter: 442\n",
            "Iter: 443\n",
            "Iter: 444\n",
            "Iter: 445\n",
            "Iter: 446\n",
            "Iter: 447\n",
            "Iter: 448\n",
            "Iter: 449\n",
            "Iter: 450\n",
            "Iter: 451\n",
            "Iter: 452\n",
            "\n",
            "Epoch: 2\n",
            "Present Rate: 2e-05\n",
            "Validation Macro: 0.4482758620689655\n",
            "Continuing on track\n",
            "Iter: 1\n",
            "Iter: 2\n",
            "Iter: 3\n",
            "Iter: 4\n",
            "Iter: 5\n",
            "Iter: 6\n",
            "Iter: 7\n",
            "Iter: 8\n",
            "Iter: 9\n",
            "Iter: 10\n",
            "Iter: 11\n",
            "Iter: 12\n",
            "Iter: 13\n",
            "Iter: 14\n",
            "Iter: 15\n",
            "Iter: 16\n",
            "Iter: 17\n",
            "Iter: 18\n",
            "Iter: 19\n",
            "Iter: 20\n",
            "Iter: 21\n",
            "Iter: 22\n",
            "Iter: 23\n",
            "Iter: 24\n",
            "Iter: 25\n",
            "Iter: 26\n",
            "Iter: 27\n",
            "Iter: 28\n",
            "Iter: 29\n",
            "Iter: 30\n",
            "Iter: 31\n",
            "Iter: 32\n",
            "Iter: 33\n",
            "Iter: 34\n",
            "Iter: 35\n",
            "Iter: 36\n",
            "Iter: 37\n",
            "Iter: 38\n",
            "Iter: 39\n",
            "Iter: 40\n",
            "Iter: 41\n",
            "Iter: 42\n",
            "Iter: 43\n",
            "Iter: 44\n",
            "Iter: 45\n",
            "Iter: 46\n",
            "Iter: 47\n",
            "Iter: 48\n",
            "Iter: 49\n",
            "Iter: 50\n",
            "Iter: 51\n",
            "Iter: 52\n",
            "Iter: 53\n",
            "Iter: 54\n",
            "Iter: 55\n",
            "Iter: 56\n",
            "Iter: 57\n",
            "Iter: 58\n",
            "Iter: 59\n",
            "Iter: 60\n",
            "Iter: 61\n",
            "Iter: 62\n",
            "Iter: 63\n",
            "Iter: 64\n",
            "Iter: 65\n",
            "Iter: 66\n",
            "Iter: 67\n",
            "Iter: 68\n",
            "Iter: 69\n",
            "Iter: 70\n",
            "Iter: 71\n",
            "Iter: 72\n",
            "Iter: 73\n",
            "Iter: 74\n",
            "Iter: 75\n",
            "Iter: 76\n",
            "Iter: 77\n",
            "Iter: 78\n",
            "Iter: 79\n",
            "Iter: 80\n",
            "Iter: 81\n",
            "Iter: 82\n",
            "Iter: 83\n",
            "Iter: 84\n",
            "Iter: 85\n",
            "Iter: 86\n",
            "Iter: 87\n",
            "Iter: 88\n",
            "Iter: 89\n",
            "Iter: 90\n",
            "Iter: 91\n",
            "Iter: 92\n",
            "Iter: 93\n",
            "Iter: 94\n",
            "Iter: 95\n",
            "Iter: 96\n",
            "Iter: 97\n",
            "Iter: 98\n",
            "Iter: 99\n",
            "Iter: 100\n",
            "Iter: 101\n",
            "Iter: 102\n",
            "Iter: 103\n",
            "Iter: 104\n",
            "Iter: 105\n",
            "Iter: 106\n",
            "Iter: 107\n",
            "Iter: 108\n",
            "Iter: 109\n",
            "Iter: 110\n",
            "Iter: 111\n",
            "Iter: 112\n",
            "Iter: 113\n",
            "Iter: 114\n",
            "Iter: 115\n",
            "Iter: 116\n",
            "Iter: 117\n",
            "Iter: 118\n",
            "Iter: 119\n",
            "Iter: 120\n",
            "Iter: 121\n",
            "Iter: 122\n",
            "Iter: 123\n",
            "Iter: 124\n",
            "Iter: 125\n",
            "Iter: 126\n",
            "Iter: 127\n",
            "Iter: 128\n",
            "Iter: 129\n",
            "Iter: 130\n",
            "Iter: 131\n",
            "Iter: 132\n",
            "Iter: 133\n",
            "Iter: 134\n",
            "Iter: 135\n",
            "Iter: 136\n",
            "Iter: 137\n",
            "Iter: 138\n",
            "Iter: 139\n",
            "Iter: 140\n",
            "Iter: 141\n",
            "Iter: 142\n",
            "Iter: 143\n",
            "Iter: 144\n",
            "Iter: 145\n",
            "Iter: 146\n",
            "Iter: 147\n",
            "Iter: 148\n",
            "Iter: 149\n",
            "Iter: 150\n",
            "Iter: 151\n",
            "Iter: 152\n",
            "Iter: 153\n",
            "Iter: 154\n",
            "Iter: 155\n",
            "Iter: 156\n",
            "Iter: 157\n",
            "Iter: 158\n",
            "Iter: 159\n",
            "Iter: 160\n",
            "Iter: 161\n",
            "Iter: 162\n",
            "Iter: 163\n",
            "Iter: 164\n",
            "Iter: 165\n",
            "Iter: 166\n",
            "Iter: 167\n",
            "Iter: 168\n",
            "Iter: 169\n",
            "Iter: 170\n",
            "Iter: 171\n",
            "Iter: 172\n",
            "Iter: 173\n",
            "Iter: 174\n",
            "Iter: 175\n",
            "Iter: 176\n",
            "Iter: 177\n",
            "Iter: 178\n",
            "Iter: 179\n",
            "Iter: 180\n",
            "Iter: 181\n",
            "Iter: 182\n",
            "Iter: 183\n",
            "Iter: 184\n",
            "Iter: 185\n",
            "Iter: 186\n",
            "Iter: 187\n",
            "Iter: 188\n",
            "Iter: 189\n",
            "Iter: 190\n",
            "Iter: 191\n",
            "Iter: 192\n",
            "Iter: 193\n",
            "Iter: 194\n",
            "Iter: 195\n",
            "Iter: 196\n",
            "Iter: 197\n",
            "Iter: 198\n",
            "Iter: 199\n",
            "Iter: 200\n",
            "Iter: 201\n",
            "Iter: 202\n",
            "Iter: 203\n",
            "Iter: 204\n",
            "Iter: 205\n",
            "Iter: 206\n",
            "Iter: 207\n",
            "Iter: 208\n",
            "Iter: 209\n",
            "Iter: 210\n",
            "Iter: 211\n",
            "Iter: 212\n",
            "Iter: 213\n",
            "Iter: 214\n",
            "Iter: 215\n",
            "Iter: 216\n",
            "Iter: 217\n",
            "Iter: 218\n",
            "Iter: 219\n",
            "Iter: 220\n",
            "Iter: 221\n",
            "Iter: 222\n",
            "Iter: 223\n",
            "Iter: 224\n",
            "Iter: 225\n",
            "Iter: 226\n",
            "Iter: 227\n",
            "Iter: 228\n",
            "Iter: 229\n",
            "Iter: 230\n",
            "Iter: 231\n",
            "Iter: 232\n",
            "Iter: 233\n",
            "Iter: 234\n",
            "Iter: 235\n",
            "Iter: 236\n",
            "Iter: 237\n",
            "Iter: 238\n",
            "Iter: 239\n",
            "Iter: 240\n",
            "Iter: 241\n",
            "Iter: 242\n",
            "Iter: 243\n",
            "Iter: 244\n",
            "Iter: 245\n",
            "Iter: 246\n",
            "Iter: 247\n",
            "Iter: 248\n",
            "Iter: 249\n",
            "Iter: 250\n",
            "Iter: 251\n",
            "Iter: 252\n",
            "Iter: 253\n",
            "Iter: 254\n",
            "Iter: 255\n",
            "Iter: 256\n",
            "Iter: 257\n",
            "Iter: 258\n",
            "Iter: 259\n",
            "Iter: 260\n",
            "Iter: 261\n",
            "Iter: 262\n",
            "Iter: 263\n",
            "Iter: 264\n",
            "Iter: 265\n",
            "Iter: 266\n",
            "Iter: 267\n",
            "Iter: 268\n",
            "Iter: 269\n",
            "Iter: 270\n",
            "Iter: 271\n",
            "Iter: 272\n",
            "Iter: 273\n",
            "Iter: 274\n",
            "Iter: 275\n",
            "Iter: 276\n",
            "Iter: 277\n",
            "Iter: 278\n",
            "Iter: 279\n",
            "Iter: 280\n",
            "Iter: 281\n",
            "Iter: 282\n",
            "Iter: 283\n",
            "Iter: 284\n",
            "Iter: 285\n",
            "Iter: 286\n",
            "Iter: 287\n",
            "Iter: 288\n",
            "Iter: 289\n",
            "Iter: 290\n",
            "Iter: 291\n",
            "Iter: 292\n",
            "Iter: 293\n",
            "Iter: 294\n",
            "Iter: 295\n",
            "Iter: 296\n",
            "Iter: 297\n",
            "Iter: 298\n",
            "Iter: 299\n",
            "Iter: 300\n",
            "Iter: 301\n",
            "Iter: 302\n",
            "Iter: 303\n",
            "Iter: 304\n",
            "Iter: 305\n",
            "Iter: 306\n",
            "Iter: 307\n",
            "Iter: 308\n",
            "Iter: 309\n",
            "Iter: 310\n",
            "Iter: 311\n",
            "Iter: 312\n",
            "Iter: 313\n",
            "Iter: 314\n",
            "Iter: 315\n",
            "Iter: 316\n",
            "Iter: 317\n",
            "Iter: 318\n",
            "Iter: 319\n",
            "Iter: 320\n",
            "Iter: 321\n",
            "Iter: 322\n",
            "Iter: 323\n",
            "Iter: 324\n",
            "Iter: 325\n",
            "Iter: 326\n",
            "Iter: 327\n",
            "Iter: 328\n",
            "Iter: 329\n",
            "Iter: 330\n",
            "Iter: 331\n",
            "Iter: 332\n",
            "Iter: 333\n",
            "Iter: 334\n",
            "Iter: 335\n",
            "Iter: 336\n",
            "Iter: 337\n",
            "Iter: 338\n",
            "Iter: 339\n",
            "Iter: 340\n",
            "Iter: 341\n",
            "Iter: 342\n",
            "Iter: 343\n",
            "Iter: 344\n",
            "Iter: 345\n",
            "Iter: 346\n",
            "Iter: 347\n",
            "Iter: 348\n",
            "Iter: 349\n",
            "Iter: 350\n",
            "Iter: 351\n",
            "Iter: 352\n",
            "Iter: 353\n",
            "Iter: 354\n",
            "Iter: 355\n",
            "Iter: 356\n",
            "Iter: 357\n",
            "Iter: 358\n",
            "Iter: 359\n",
            "Iter: 360\n",
            "Iter: 361\n",
            "Iter: 362\n",
            "Iter: 363\n",
            "Iter: 364\n",
            "Iter: 365\n",
            "Iter: 366\n",
            "Iter: 367\n",
            "Iter: 368\n",
            "Iter: 369\n",
            "Iter: 370\n",
            "Iter: 371\n",
            "Iter: 372\n",
            "Iter: 373\n",
            "Iter: 374\n",
            "Iter: 375\n",
            "Iter: 376\n",
            "Iter: 377\n",
            "Iter: 378\n",
            "Iter: 379\n",
            "Iter: 380\n",
            "Iter: 381\n",
            "Iter: 382\n",
            "Iter: 383\n",
            "Iter: 384\n",
            "Iter: 385\n",
            "Iter: 386\n",
            "Iter: 387\n",
            "Iter: 388\n",
            "Iter: 389\n",
            "Iter: 390\n",
            "Iter: 391\n",
            "Iter: 392\n",
            "Iter: 393\n",
            "Iter: 394\n",
            "Iter: 395\n",
            "Iter: 396\n",
            "Iter: 397\n",
            "Iter: 398\n",
            "Iter: 399\n",
            "Iter: 400\n",
            "Iter: 401\n",
            "Iter: 402\n",
            "Iter: 403\n",
            "Iter: 404\n",
            "Iter: 405\n",
            "Iter: 406\n",
            "Iter: 407\n",
            "Iter: 408\n",
            "Iter: 409\n",
            "Iter: 410\n",
            "Iter: 411\n",
            "Iter: 412\n",
            "Iter: 413\n",
            "Iter: 414\n",
            "Iter: 415\n",
            "Iter: 416\n",
            "Iter: 417\n",
            "Iter: 418\n",
            "Iter: 419\n",
            "Iter: 420\n",
            "Iter: 421\n",
            "Iter: 422\n",
            "Iter: 423\n",
            "Iter: 424\n",
            "Iter: 425\n",
            "Iter: 426\n",
            "Iter: 427\n",
            "Iter: 428\n",
            "Iter: 429\n",
            "Iter: 430\n",
            "Iter: 431\n",
            "Iter: 432\n",
            "Iter: 433\n",
            "Iter: 434\n",
            "Iter: 435\n",
            "Iter: 436\n",
            "Iter: 437\n",
            "Iter: 438\n",
            "Iter: 439\n",
            "Iter: 440\n",
            "Iter: 441\n",
            "Iter: 442\n",
            "Iter: 443\n",
            "Iter: 444\n",
            "Iter: 445\n",
            "Iter: 446\n",
            "Iter: 447\n",
            "Iter: 448\n",
            "Iter: 449\n",
            "Iter: 450\n",
            "Iter: 451\n",
            "Iter: 452\n",
            "\n",
            "Epoch: 3\n",
            "Present Rate: 2e-05\n",
            "Validation Macro: 0.7948717948717949\n",
            "Continuing on track\n",
            "Iter: 1\n",
            "Iter: 2\n",
            "Iter: 3\n",
            "Iter: 4\n",
            "Iter: 5\n",
            "Iter: 6\n",
            "Iter: 7\n",
            "Iter: 8\n",
            "Iter: 9\n",
            "Iter: 10\n",
            "Iter: 11\n",
            "Iter: 12\n",
            "Iter: 13\n",
            "Iter: 14\n",
            "Iter: 15\n",
            "Iter: 16\n",
            "Iter: 17\n",
            "Iter: 18\n",
            "Iter: 19\n",
            "Iter: 20\n",
            "Iter: 21\n",
            "Iter: 22\n",
            "Iter: 23\n",
            "Iter: 24\n",
            "Iter: 25\n",
            "Iter: 26\n",
            "Iter: 27\n",
            "Iter: 28\n",
            "Iter: 29\n",
            "Iter: 30\n",
            "Iter: 31\n",
            "Iter: 32\n",
            "Iter: 33\n",
            "Iter: 34\n",
            "Iter: 35\n",
            "Iter: 36\n",
            "Iter: 37\n",
            "Iter: 38\n",
            "Iter: 39\n",
            "Iter: 40\n",
            "Iter: 41\n",
            "Iter: 42\n",
            "Iter: 43\n",
            "Iter: 44\n",
            "Iter: 45\n",
            "Iter: 46\n",
            "Iter: 47\n",
            "Iter: 48\n",
            "Iter: 49\n",
            "Iter: 50\n",
            "Iter: 51\n",
            "Iter: 52\n",
            "Iter: 53\n",
            "Iter: 54\n",
            "Iter: 55\n",
            "Iter: 56\n",
            "Iter: 57\n",
            "Iter: 58\n",
            "Iter: 59\n",
            "Iter: 60\n",
            "Iter: 61\n",
            "Iter: 62\n",
            "Iter: 63\n",
            "Iter: 64\n",
            "Iter: 65\n",
            "Iter: 66\n",
            "Iter: 67\n",
            "Iter: 68\n",
            "Iter: 69\n",
            "Iter: 70\n",
            "Iter: 71\n",
            "Iter: 72\n",
            "Iter: 73\n",
            "Iter: 74\n",
            "Iter: 75\n",
            "Iter: 76\n",
            "Iter: 77\n",
            "Iter: 78\n",
            "Iter: 79\n",
            "Iter: 80\n",
            "Iter: 81\n",
            "Iter: 82\n",
            "Iter: 83\n",
            "Iter: 84\n",
            "Iter: 85\n",
            "Iter: 86\n",
            "Iter: 87\n",
            "Iter: 88\n",
            "Iter: 89\n",
            "Iter: 90\n",
            "Iter: 91\n",
            "Iter: 92\n",
            "Iter: 93\n",
            "Iter: 94\n",
            "Iter: 95\n",
            "Iter: 96\n",
            "Iter: 97\n",
            "Iter: 98\n",
            "Iter: 99\n",
            "Iter: 100\n",
            "Iter: 101\n",
            "Iter: 102\n",
            "Iter: 103\n",
            "Iter: 104\n",
            "Iter: 105\n",
            "Iter: 106\n",
            "Iter: 107\n",
            "Iter: 108\n",
            "Iter: 109\n",
            "Iter: 110\n",
            "Iter: 111\n",
            "Iter: 112\n",
            "Iter: 113\n",
            "Iter: 114\n",
            "Iter: 115\n",
            "Iter: 116\n",
            "Iter: 117\n",
            "Iter: 118\n",
            "Iter: 119\n",
            "Iter: 120\n",
            "Iter: 121\n",
            "Iter: 122\n",
            "Iter: 123\n",
            "Iter: 124\n",
            "Iter: 125\n",
            "Iter: 126\n",
            "Iter: 127\n",
            "Iter: 128\n",
            "Iter: 129\n",
            "Iter: 130\n",
            "Iter: 131\n",
            "Iter: 132\n",
            "Iter: 133\n",
            "Iter: 134\n",
            "Iter: 135\n",
            "Iter: 136\n",
            "Iter: 137\n",
            "Iter: 138\n",
            "Iter: 139\n",
            "Iter: 140\n",
            "Iter: 141\n",
            "Iter: 142\n",
            "Iter: 143\n",
            "Iter: 144\n",
            "Iter: 145\n",
            "Iter: 146\n",
            "Iter: 147\n",
            "Iter: 148\n",
            "Iter: 149\n",
            "Iter: 150\n",
            "Iter: 151\n",
            "Iter: 152\n",
            "Iter: 153\n",
            "Iter: 154\n",
            "Iter: 155\n",
            "Iter: 156\n",
            "Iter: 157\n",
            "Iter: 158\n",
            "Iter: 159\n",
            "Iter: 160\n",
            "Iter: 161\n",
            "Iter: 162\n",
            "Iter: 163\n",
            "Iter: 164\n",
            "Iter: 165\n",
            "Iter: 166\n",
            "Iter: 167\n",
            "Iter: 168\n",
            "Iter: 169\n",
            "Iter: 170\n",
            "Iter: 171\n",
            "Iter: 172\n",
            "Iter: 173\n",
            "Iter: 174\n",
            "Iter: 175\n",
            "Iter: 176\n",
            "Iter: 177\n",
            "Iter: 178\n",
            "Iter: 179\n",
            "Iter: 180\n",
            "Iter: 181\n",
            "Iter: 182\n",
            "Iter: 183\n",
            "Iter: 184\n",
            "Iter: 185\n",
            "Iter: 186\n",
            "Iter: 187\n",
            "Iter: 188\n",
            "Iter: 189\n",
            "Iter: 190\n",
            "Iter: 191\n",
            "Iter: 192\n",
            "Iter: 193\n",
            "Iter: 194\n",
            "Iter: 195\n",
            "Iter: 196\n",
            "Iter: 197\n",
            "Iter: 198\n",
            "Iter: 199\n",
            "Iter: 200\n",
            "Iter: 201\n",
            "Iter: 202\n",
            "Iter: 203\n",
            "Iter: 204\n",
            "Iter: 205\n",
            "Iter: 206\n",
            "Iter: 207\n",
            "Iter: 208\n",
            "Iter: 209\n",
            "Iter: 210\n",
            "Iter: 211\n",
            "Iter: 212\n",
            "Iter: 213\n",
            "Iter: 214\n",
            "Iter: 215\n",
            "Iter: 216\n",
            "Iter: 217\n",
            "Iter: 218\n",
            "Iter: 219\n",
            "Iter: 220\n",
            "Iter: 221\n",
            "Iter: 222\n",
            "Iter: 223\n",
            "Iter: 224\n",
            "Iter: 225\n",
            "Iter: 226\n",
            "Iter: 227\n",
            "Iter: 228\n",
            "Iter: 229\n",
            "Iter: 230\n",
            "Iter: 231\n",
            "Iter: 232\n",
            "Iter: 233\n",
            "Iter: 234\n",
            "Iter: 235\n",
            "Iter: 236\n",
            "Iter: 237\n",
            "Iter: 238\n",
            "Iter: 239\n",
            "Iter: 240\n",
            "Iter: 241\n",
            "Iter: 242\n",
            "Iter: 243\n",
            "Iter: 244\n",
            "Iter: 245\n",
            "Iter: 246\n",
            "Iter: 247\n",
            "Iter: 248\n",
            "Iter: 249\n",
            "Iter: 250\n",
            "Iter: 251\n",
            "Iter: 252\n",
            "Iter: 253\n",
            "Iter: 254\n",
            "Iter: 255\n",
            "Iter: 256\n",
            "Iter: 257\n",
            "Iter: 258\n",
            "Iter: 259\n",
            "Iter: 260\n",
            "Iter: 261\n",
            "Iter: 262\n",
            "Iter: 263\n",
            "Iter: 264\n",
            "Iter: 265\n",
            "Iter: 266\n",
            "Iter: 267\n",
            "Iter: 268\n",
            "Iter: 269\n",
            "Iter: 270\n",
            "Iter: 271\n",
            "Iter: 272\n",
            "Iter: 273\n",
            "Iter: 274\n",
            "Iter: 275\n",
            "Iter: 276\n",
            "Iter: 277\n",
            "Iter: 278\n",
            "Iter: 279\n",
            "Iter: 280\n",
            "Iter: 281\n",
            "Iter: 282\n",
            "Iter: 283\n",
            "Iter: 284\n",
            "Iter: 285\n",
            "Iter: 286\n",
            "Iter: 287\n",
            "Iter: 288\n",
            "Iter: 289\n",
            "Iter: 290\n",
            "Iter: 291\n",
            "Iter: 292\n",
            "Iter: 293\n",
            "Iter: 294\n",
            "Iter: 295\n",
            "Iter: 296\n",
            "Iter: 297\n",
            "Iter: 298\n",
            "Iter: 299\n",
            "Iter: 300\n",
            "Iter: 301\n",
            "Iter: 302\n",
            "Iter: 303\n",
            "Iter: 304\n",
            "Iter: 305\n",
            "Iter: 306\n",
            "Iter: 307\n",
            "Iter: 308\n",
            "Iter: 309\n",
            "Iter: 310\n",
            "Iter: 311\n",
            "Iter: 312\n",
            "Iter: 313\n",
            "Iter: 314\n",
            "Iter: 315\n",
            "Iter: 316\n",
            "Iter: 317\n",
            "Iter: 318\n",
            "Iter: 319\n",
            "Iter: 320\n",
            "Iter: 321\n",
            "Iter: 322\n",
            "Iter: 323\n",
            "Iter: 324\n",
            "Iter: 325\n",
            "Iter: 326\n",
            "Iter: 327\n",
            "Iter: 328\n",
            "Iter: 329\n",
            "Iter: 330\n",
            "Iter: 331\n",
            "Iter: 332\n",
            "Iter: 333\n",
            "Iter: 334\n",
            "Iter: 335\n",
            "Iter: 336\n",
            "Iter: 337\n",
            "Iter: 338\n",
            "Iter: 339\n",
            "Iter: 340\n",
            "Iter: 341\n",
            "Iter: 342\n",
            "Iter: 343\n",
            "Iter: 344\n",
            "Iter: 345\n",
            "Iter: 346\n",
            "Iter: 347\n",
            "Iter: 348\n",
            "Iter: 349\n",
            "Iter: 350\n",
            "Iter: 351\n",
            "Iter: 352\n",
            "Iter: 353\n",
            "Iter: 354\n",
            "Iter: 355\n",
            "Iter: 356\n",
            "Iter: 357\n",
            "Iter: 358\n",
            "Iter: 359\n",
            "Iter: 360\n",
            "Iter: 361\n",
            "Iter: 362\n",
            "Iter: 363\n",
            "Iter: 364\n",
            "Iter: 365\n",
            "Iter: 366\n",
            "Iter: 367\n",
            "Iter: 368\n",
            "Iter: 369\n",
            "Iter: 370\n",
            "Iter: 371\n",
            "Iter: 372\n",
            "Iter: 373\n",
            "Iter: 374\n",
            "Iter: 375\n",
            "Iter: 376\n",
            "Iter: 377\n",
            "Iter: 378\n",
            "Iter: 379\n",
            "Iter: 380\n",
            "Iter: 381\n",
            "Iter: 382\n",
            "Iter: 383\n",
            "Iter: 384\n",
            "Iter: 385\n",
            "Iter: 386\n",
            "Iter: 387\n",
            "Iter: 388\n",
            "Iter: 389\n",
            "Iter: 390\n",
            "Iter: 391\n",
            "Iter: 392\n",
            "Iter: 393\n",
            "Iter: 394\n",
            "Iter: 395\n",
            "Iter: 396\n",
            "Iter: 397\n",
            "Iter: 398\n",
            "Iter: 399\n",
            "Iter: 400\n",
            "Iter: 401\n",
            "Iter: 402\n",
            "Iter: 403\n",
            "Iter: 404\n",
            "Iter: 405\n",
            "Iter: 406\n",
            "Iter: 407\n",
            "Iter: 408\n",
            "Iter: 409\n",
            "Iter: 410\n",
            "Iter: 411\n",
            "Iter: 412\n",
            "Iter: 413\n",
            "Iter: 414\n",
            "Iter: 415\n",
            "Iter: 416\n",
            "Iter: 417\n",
            "Iter: 418\n",
            "Iter: 419\n",
            "Iter: 420\n",
            "Iter: 421\n",
            "Iter: 422\n",
            "Iter: 423\n",
            "Iter: 424\n",
            "Iter: 425\n",
            "Iter: 426\n",
            "Iter: 427\n",
            "Iter: 428\n",
            "Iter: 429\n",
            "Iter: 430\n",
            "Iter: 431\n",
            "Iter: 432\n",
            "Iter: 433\n",
            "Iter: 434\n",
            "Iter: 435\n",
            "Iter: 436\n",
            "Iter: 437\n",
            "Iter: 438\n",
            "Iter: 439\n",
            "Iter: 440\n",
            "Iter: 441\n",
            "Iter: 442\n",
            "Iter: 443\n",
            "Iter: 444\n",
            "Iter: 445\n",
            "Iter: 446\n",
            "Iter: 447\n",
            "Iter: 448\n",
            "Iter: 449\n",
            "Iter: 450\n",
            "Iter: 451\n",
            "Iter: 452\n",
            "\n",
            "Epoch: 4\n",
            "Present Rate: 2e-05\n",
            "Validation Macro: 0.7948717948717949\n",
            "Backtrack\n",
            "Iter: 1\n",
            "Iter: 2\n",
            "Iter: 3\n",
            "Iter: 4\n",
            "Iter: 5\n",
            "Iter: 6\n",
            "Iter: 7\n",
            "Iter: 8\n",
            "Iter: 9\n",
            "Iter: 10\n",
            "Iter: 11\n",
            "Iter: 12\n",
            "Iter: 13\n",
            "Iter: 14\n",
            "Iter: 15\n",
            "Iter: 16\n",
            "Iter: 17\n",
            "Iter: 18\n",
            "Iter: 19\n",
            "Iter: 20\n",
            "Iter: 21\n",
            "Iter: 22\n",
            "Iter: 23\n",
            "Iter: 24\n",
            "Iter: 25\n",
            "Iter: 26\n",
            "Iter: 27\n",
            "Iter: 28\n",
            "Iter: 29\n",
            "Iter: 30\n",
            "Iter: 31\n",
            "Iter: 32\n",
            "Iter: 33\n",
            "Iter: 34\n",
            "Iter: 35\n",
            "Iter: 36\n",
            "Iter: 37\n",
            "Iter: 38\n",
            "Iter: 39\n",
            "Iter: 40\n",
            "Iter: 41\n",
            "Iter: 42\n",
            "Iter: 43\n",
            "Iter: 44\n",
            "Iter: 45\n",
            "Iter: 46\n",
            "Iter: 47\n",
            "Iter: 48\n",
            "Iter: 49\n",
            "Iter: 50\n",
            "Iter: 51\n",
            "Iter: 52\n",
            "Iter: 53\n",
            "Iter: 54\n",
            "Iter: 55\n",
            "Iter: 56\n",
            "Iter: 57\n",
            "Iter: 58\n",
            "Iter: 59\n",
            "Iter: 60\n",
            "Iter: 61\n",
            "Iter: 62\n",
            "Iter: 63\n",
            "Iter: 64\n",
            "Iter: 65\n",
            "Iter: 66\n",
            "Iter: 67\n",
            "Iter: 68\n",
            "Iter: 69\n",
            "Iter: 70\n",
            "Iter: 71\n",
            "Iter: 72\n",
            "Iter: 73\n",
            "Iter: 74\n",
            "Iter: 75\n",
            "Iter: 76\n",
            "Iter: 77\n",
            "Iter: 78\n",
            "Iter: 79\n",
            "Iter: 80\n",
            "Iter: 81\n",
            "Iter: 82\n",
            "Iter: 83\n",
            "Iter: 84\n",
            "Iter: 85\n",
            "Iter: 86\n",
            "Iter: 87\n",
            "Iter: 88\n",
            "Iter: 89\n",
            "Iter: 90\n",
            "Iter: 91\n",
            "Iter: 92\n",
            "Iter: 93\n",
            "Iter: 94\n",
            "Iter: 95\n",
            "Iter: 96\n",
            "Iter: 97\n",
            "Iter: 98\n",
            "Iter: 99\n",
            "Iter: 100\n",
            "Iter: 101\n",
            "Iter: 102\n",
            "Iter: 103\n",
            "Iter: 104\n",
            "Iter: 105\n",
            "Iter: 106\n",
            "Iter: 107\n",
            "Iter: 108\n",
            "Iter: 109\n",
            "Iter: 110\n",
            "Iter: 111\n",
            "Iter: 112\n",
            "Iter: 113\n",
            "Iter: 114\n",
            "Iter: 115\n",
            "Iter: 116\n",
            "Iter: 117\n",
            "Iter: 118\n",
            "Iter: 119\n",
            "Iter: 120\n",
            "Iter: 121\n",
            "Iter: 122\n",
            "Iter: 123\n",
            "Iter: 124\n",
            "Iter: 125\n",
            "Iter: 126\n",
            "Iter: 127\n",
            "Iter: 128\n",
            "Iter: 129\n",
            "Iter: 130\n",
            "Iter: 131\n",
            "Iter: 132\n",
            "Iter: 133\n",
            "Iter: 134\n",
            "Iter: 135\n",
            "Iter: 136\n",
            "Iter: 137\n",
            "Iter: 138\n",
            "Iter: 139\n",
            "Iter: 140\n",
            "Iter: 141\n",
            "Iter: 142\n",
            "Iter: 143\n",
            "Iter: 144\n",
            "Iter: 145\n",
            "Iter: 146\n",
            "Iter: 147\n",
            "Iter: 148\n",
            "Iter: 149\n",
            "Iter: 150\n",
            "Iter: 151\n",
            "Iter: 152\n",
            "Iter: 153\n",
            "Iter: 154\n",
            "Iter: 155\n",
            "Iter: 156\n",
            "Iter: 157\n",
            "Iter: 158\n",
            "Iter: 159\n",
            "Iter: 160\n",
            "Iter: 161\n",
            "Iter: 162\n",
            "Iter: 163\n",
            "Iter: 164\n",
            "Iter: 165\n",
            "Iter: 166\n",
            "Iter: 167\n",
            "Iter: 168\n",
            "Iter: 169\n",
            "Iter: 170\n",
            "Iter: 171\n",
            "Iter: 172\n",
            "Iter: 173\n",
            "Iter: 174\n",
            "Iter: 175\n",
            "Iter: 176\n",
            "Iter: 177\n",
            "Iter: 178\n",
            "Iter: 179\n",
            "Iter: 180\n",
            "Iter: 181\n",
            "Iter: 182\n",
            "Iter: 183\n",
            "Iter: 184\n",
            "Iter: 185\n",
            "Iter: 186\n",
            "Iter: 187\n",
            "Iter: 188\n",
            "Iter: 189\n",
            "Iter: 190\n",
            "Iter: 191\n",
            "Iter: 192\n",
            "Iter: 193\n",
            "Iter: 194\n",
            "Iter: 195\n",
            "Iter: 196\n",
            "Iter: 197\n",
            "Iter: 198\n",
            "Iter: 199\n",
            "Iter: 200\n",
            "Iter: 201\n",
            "Iter: 202\n",
            "Iter: 203\n",
            "Iter: 204\n",
            "Iter: 205\n",
            "Iter: 206\n",
            "Iter: 207\n",
            "Iter: 208\n",
            "Iter: 209\n",
            "Iter: 210\n",
            "Iter: 211\n",
            "Iter: 212\n",
            "Iter: 213\n",
            "Iter: 214\n",
            "Iter: 215\n",
            "Iter: 216\n",
            "Iter: 217\n",
            "Iter: 218\n",
            "Iter: 219\n",
            "Iter: 220\n",
            "Iter: 221\n",
            "Iter: 222\n",
            "Iter: 223\n",
            "Iter: 224\n",
            "Iter: 225\n",
            "Iter: 226\n",
            "Iter: 227\n",
            "Iter: 228\n",
            "Iter: 229\n",
            "Iter: 230\n",
            "Iter: 231\n",
            "Iter: 232\n",
            "Iter: 233\n",
            "Iter: 234\n",
            "Iter: 235\n",
            "Iter: 236\n",
            "Iter: 237\n",
            "Iter: 238\n",
            "Iter: 239\n",
            "Iter: 240\n",
            "Iter: 241\n",
            "Iter: 242\n",
            "Iter: 243\n",
            "Iter: 244\n",
            "Iter: 245\n",
            "Iter: 246\n",
            "Iter: 247\n",
            "Iter: 248\n",
            "Iter: 249\n",
            "Iter: 250\n",
            "Iter: 251\n",
            "Iter: 252\n",
            "Iter: 253\n",
            "Iter: 254\n",
            "Iter: 255\n",
            "Iter: 256\n",
            "Iter: 257\n",
            "Iter: 258\n",
            "Iter: 259\n",
            "Iter: 260\n",
            "Iter: 261\n",
            "Iter: 262\n",
            "Iter: 263\n",
            "Iter: 264\n",
            "Iter: 265\n",
            "Iter: 266\n",
            "Iter: 267\n",
            "Iter: 268\n",
            "Iter: 269\n",
            "Iter: 270\n",
            "Iter: 271\n",
            "Iter: 272\n",
            "Iter: 273\n",
            "Iter: 274\n",
            "Iter: 275\n",
            "Iter: 276\n",
            "Iter: 277\n",
            "Iter: 278\n",
            "Iter: 279\n",
            "Iter: 280\n",
            "Iter: 281\n",
            "Iter: 282\n",
            "Iter: 283\n",
            "Iter: 284\n",
            "Iter: 285\n",
            "Iter: 286\n",
            "Iter: 287\n",
            "Iter: 288\n",
            "Iter: 289\n",
            "Iter: 290\n",
            "Iter: 291\n",
            "Iter: 292\n",
            "Iter: 293\n",
            "Iter: 294\n",
            "Iter: 295\n",
            "Iter: 296\n",
            "Iter: 297\n",
            "Iter: 298\n",
            "Iter: 299\n",
            "Iter: 300\n",
            "Iter: 301\n",
            "Iter: 302\n",
            "Iter: 303\n",
            "Iter: 304\n",
            "Iter: 305\n",
            "Iter: 306\n",
            "Iter: 307\n",
            "Iter: 308\n",
            "Iter: 309\n",
            "Iter: 310\n",
            "Iter: 311\n",
            "Iter: 312\n",
            "Iter: 313\n",
            "Iter: 314\n",
            "Iter: 315\n",
            "Iter: 316\n",
            "Iter: 317\n",
            "Iter: 318\n",
            "Iter: 319\n",
            "Iter: 320\n",
            "Iter: 321\n",
            "Iter: 322\n",
            "Iter: 323\n",
            "Iter: 324\n",
            "Iter: 325\n",
            "Iter: 326\n",
            "Iter: 327\n",
            "Iter: 328\n",
            "Iter: 329\n",
            "Iter: 330\n",
            "Iter: 331\n",
            "Iter: 332\n",
            "Iter: 333\n",
            "Iter: 334\n",
            "Iter: 335\n",
            "Iter: 336\n",
            "Iter: 337\n",
            "Iter: 338\n",
            "Iter: 339\n",
            "Iter: 340\n",
            "Iter: 341\n",
            "Iter: 342\n",
            "Iter: 343\n",
            "Iter: 344\n",
            "Iter: 345\n",
            "Iter: 346\n",
            "Iter: 347\n",
            "Iter: 348\n",
            "Iter: 349\n",
            "Iter: 350\n",
            "Iter: 351\n",
            "Iter: 352\n",
            "Iter: 353\n",
            "Iter: 354\n",
            "Iter: 355\n",
            "Iter: 356\n",
            "Iter: 357\n",
            "Iter: 358\n",
            "Iter: 359\n",
            "Iter: 360\n",
            "Iter: 361\n",
            "Iter: 362\n",
            "Iter: 363\n",
            "Iter: 364\n",
            "Iter: 365\n",
            "Iter: 366\n",
            "Iter: 367\n",
            "Iter: 368\n",
            "Iter: 369\n",
            "Iter: 370\n",
            "Iter: 371\n",
            "Iter: 372\n",
            "Iter: 373\n",
            "Iter: 374\n",
            "Iter: 375\n",
            "Iter: 376\n",
            "Iter: 377\n",
            "Iter: 378\n",
            "Iter: 379\n",
            "Iter: 380\n",
            "Iter: 381\n",
            "Iter: 382\n",
            "Iter: 383\n",
            "Iter: 384\n",
            "Iter: 385\n",
            "Iter: 386\n",
            "Iter: 387\n",
            "Iter: 388\n",
            "Iter: 389\n",
            "Iter: 390\n",
            "Iter: 391\n",
            "Iter: 392\n",
            "Iter: 393\n",
            "Iter: 394\n",
            "Iter: 395\n",
            "Iter: 396\n",
            "Iter: 397\n",
            "Iter: 398\n",
            "Iter: 399\n",
            "Iter: 400\n",
            "Iter: 401\n",
            "Iter: 402\n",
            "Iter: 403\n",
            "Iter: 404\n",
            "Iter: 405\n",
            "Iter: 406\n",
            "Iter: 407\n",
            "Iter: 408\n",
            "Iter: 409\n",
            "Iter: 410\n",
            "Iter: 411\n",
            "Iter: 412\n",
            "Iter: 413\n",
            "Iter: 414\n",
            "Iter: 415\n",
            "Iter: 416\n",
            "Iter: 417\n",
            "Iter: 418\n",
            "Iter: 419\n",
            "Iter: 420\n",
            "Iter: 421\n",
            "Iter: 422\n",
            "Iter: 423\n",
            "Iter: 424\n",
            "Iter: 425\n",
            "Iter: 426\n",
            "Iter: 427\n",
            "Iter: 428\n",
            "Iter: 429\n",
            "Iter: 430\n",
            "Iter: 431\n",
            "Iter: 432\n",
            "Iter: 433\n",
            "Iter: 434\n",
            "Iter: 435\n",
            "Iter: 436\n",
            "Iter: 437\n",
            "Iter: 438\n",
            "Iter: 439\n",
            "Iter: 440\n",
            "Iter: 441\n",
            "Iter: 442\n",
            "Iter: 443\n",
            "Iter: 444\n",
            "Iter: 445\n",
            "Iter: 446\n",
            "Iter: 447\n",
            "Iter: 448\n",
            "Iter: 449\n",
            "Iter: 450\n",
            "Iter: 451\n",
            "Iter: 452\n",
            "\n",
            "Epoch: 5\n",
            "Present Rate: 5e-06\n",
            "Validation Macro: 0.7948717948717949\n",
            "Backtrack\n",
            "Iter: 1\n",
            "Iter: 2\n",
            "Iter: 3\n",
            "Iter: 4\n",
            "Iter: 5\n",
            "Iter: 6\n",
            "Iter: 7\n",
            "Iter: 8\n",
            "Iter: 9\n",
            "Iter: 10\n",
            "Iter: 11\n",
            "Iter: 12\n",
            "Iter: 13\n",
            "Iter: 14\n",
            "Iter: 15\n",
            "Iter: 16\n",
            "Iter: 17\n",
            "Iter: 18\n",
            "Iter: 19\n",
            "Iter: 20\n",
            "Iter: 21\n",
            "Iter: 22\n",
            "Iter: 23\n",
            "Iter: 24\n",
            "Iter: 25\n",
            "Iter: 26\n",
            "Iter: 27\n",
            "Iter: 28\n",
            "Iter: 29\n",
            "Iter: 30\n",
            "Iter: 31\n",
            "Iter: 32\n",
            "Iter: 33\n",
            "Iter: 34\n",
            "Iter: 35\n",
            "Iter: 36\n",
            "Iter: 37\n",
            "Iter: 38\n",
            "Iter: 39\n",
            "Iter: 40\n",
            "Iter: 41\n",
            "Iter: 42\n",
            "Iter: 43\n",
            "Iter: 44\n",
            "Iter: 45\n",
            "Iter: 46\n",
            "Iter: 47\n",
            "Iter: 48\n",
            "Iter: 49\n",
            "Iter: 50\n",
            "Iter: 51\n",
            "Iter: 52\n",
            "Iter: 53\n",
            "Iter: 54\n",
            "Iter: 55\n",
            "Iter: 56\n",
            "Iter: 57\n",
            "Iter: 58\n",
            "Iter: 59\n",
            "Iter: 60\n",
            "Iter: 61\n",
            "Iter: 62\n",
            "Iter: 63\n",
            "Iter: 64\n",
            "Iter: 65\n",
            "Iter: 66\n",
            "Iter: 67\n",
            "Iter: 68\n",
            "Iter: 69\n",
            "Iter: 70\n",
            "Iter: 71\n",
            "Iter: 72\n",
            "Iter: 73\n",
            "Iter: 74\n",
            "Iter: 75\n",
            "Iter: 76\n",
            "Iter: 77\n",
            "Iter: 78\n",
            "Iter: 79\n",
            "Iter: 80\n",
            "Iter: 81\n",
            "Iter: 82\n",
            "Iter: 83\n",
            "Iter: 84\n",
            "Iter: 85\n",
            "Iter: 86\n",
            "Iter: 87\n",
            "Iter: 88\n",
            "Iter: 89\n",
            "Iter: 90\n",
            "Iter: 91\n",
            "Iter: 92\n",
            "Iter: 93\n",
            "Iter: 94\n",
            "Iter: 95\n",
            "Iter: 96\n",
            "Iter: 97\n",
            "Iter: 98\n",
            "Iter: 99\n",
            "Iter: 100\n",
            "Iter: 101\n",
            "Iter: 102\n",
            "Iter: 103\n",
            "Iter: 104\n",
            "Iter: 105\n",
            "Iter: 106\n",
            "Iter: 107\n",
            "Iter: 108\n",
            "Iter: 109\n",
            "Iter: 110\n",
            "Iter: 111\n",
            "Iter: 112\n",
            "Iter: 113\n",
            "Iter: 114\n",
            "Iter: 115\n",
            "Iter: 116\n",
            "Iter: 117\n",
            "Iter: 118\n",
            "Iter: 119\n",
            "Iter: 120\n",
            "Iter: 121\n",
            "Iter: 122\n",
            "Iter: 123\n",
            "Iter: 124\n",
            "Iter: 125\n",
            "Iter: 126\n",
            "Iter: 127\n",
            "Iter: 128\n",
            "Iter: 129\n",
            "Iter: 130\n",
            "Iter: 131\n",
            "Iter: 132\n",
            "Iter: 133\n",
            "Iter: 134\n",
            "Iter: 135\n",
            "Iter: 136\n",
            "Iter: 137\n",
            "Iter: 138\n",
            "Iter: 139\n",
            "Iter: 140\n",
            "Iter: 141\n",
            "Iter: 142\n",
            "Iter: 143\n",
            "Iter: 144\n",
            "Iter: 145\n",
            "Iter: 146\n",
            "Iter: 147\n",
            "Iter: 148\n",
            "Iter: 149\n",
            "Iter: 150\n",
            "Iter: 151\n",
            "Iter: 152\n",
            "Iter: 153\n",
            "Iter: 154\n",
            "Iter: 155\n",
            "Iter: 156\n",
            "Iter: 157\n",
            "Iter: 158\n",
            "Iter: 159\n",
            "Iter: 160\n",
            "Iter: 161\n",
            "Iter: 162\n",
            "Iter: 163\n",
            "Iter: 164\n",
            "Iter: 165\n",
            "Iter: 166\n",
            "Iter: 167\n",
            "Iter: 168\n",
            "Iter: 169\n",
            "Iter: 170\n",
            "Iter: 171\n",
            "Iter: 172\n",
            "Iter: 173\n",
            "Iter: 174\n",
            "Iter: 175\n",
            "Iter: 176\n",
            "Iter: 177\n",
            "Iter: 178\n",
            "Iter: 179\n",
            "Iter: 180\n",
            "Iter: 181\n",
            "Iter: 182\n",
            "Iter: 183\n",
            "Iter: 184\n",
            "Iter: 185\n",
            "Iter: 186\n",
            "Iter: 187\n",
            "Iter: 188\n",
            "Iter: 189\n",
            "Iter: 190\n",
            "Iter: 191\n",
            "Iter: 192\n",
            "Iter: 193\n",
            "Iter: 194\n",
            "Iter: 195\n",
            "Iter: 196\n",
            "Iter: 197\n",
            "Iter: 198\n",
            "Iter: 199\n",
            "Iter: 200\n",
            "Iter: 201\n",
            "Iter: 202\n",
            "Iter: 203\n",
            "Iter: 204\n",
            "Iter: 205\n",
            "Iter: 206\n",
            "Iter: 207\n",
            "Iter: 208\n",
            "Iter: 209\n",
            "Iter: 210\n",
            "Iter: 211\n",
            "Iter: 212\n",
            "Iter: 213\n",
            "Iter: 214\n",
            "Iter: 215\n",
            "Iter: 216\n",
            "Iter: 217\n",
            "Iter: 218\n",
            "Iter: 219\n",
            "Iter: 220\n",
            "Iter: 221\n",
            "Iter: 222\n",
            "Iter: 223\n",
            "Iter: 224\n",
            "Iter: 225\n",
            "Iter: 226\n",
            "Iter: 227\n",
            "Iter: 228\n",
            "Iter: 229\n",
            "Iter: 230\n",
            "Iter: 231\n",
            "Iter: 232\n",
            "Iter: 233\n",
            "Iter: 234\n",
            "Iter: 235\n",
            "Iter: 236\n",
            "Iter: 237\n",
            "Iter: 238\n",
            "Iter: 239\n",
            "Iter: 240\n",
            "Iter: 241\n",
            "Iter: 242\n",
            "Iter: 243\n",
            "Iter: 244\n",
            "Iter: 245\n",
            "Iter: 246\n",
            "Iter: 247\n",
            "Iter: 248\n",
            "Iter: 249\n",
            "Iter: 250\n",
            "Iter: 251\n",
            "Iter: 252\n",
            "Iter: 253\n",
            "Iter: 254\n",
            "Iter: 255\n",
            "Iter: 256\n",
            "Iter: 257\n",
            "Iter: 258\n",
            "Iter: 259\n",
            "Iter: 260\n",
            "Iter: 261\n",
            "Iter: 262\n",
            "Iter: 263\n",
            "Iter: 264\n",
            "Iter: 265\n",
            "Iter: 266\n",
            "Iter: 267\n",
            "Iter: 268\n",
            "Iter: 269\n",
            "Iter: 270\n",
            "Iter: 271\n",
            "Iter: 272\n",
            "Iter: 273\n",
            "Iter: 274\n",
            "Iter: 275\n",
            "Iter: 276\n",
            "Iter: 277\n",
            "Iter: 278\n",
            "Iter: 279\n",
            "Iter: 280\n",
            "Iter: 281\n",
            "Iter: 282\n",
            "Iter: 283\n",
            "Iter: 284\n",
            "Iter: 285\n",
            "Iter: 286\n",
            "Iter: 287\n",
            "Iter: 288\n",
            "Iter: 289\n",
            "Iter: 290\n",
            "Iter: 291\n",
            "Iter: 292\n",
            "Iter: 293\n",
            "Iter: 294\n",
            "Iter: 295\n",
            "Iter: 296\n",
            "Iter: 297\n",
            "Iter: 298\n",
            "Iter: 299\n",
            "Iter: 300\n",
            "Iter: 301\n",
            "Iter: 302\n",
            "Iter: 303\n",
            "Iter: 304\n",
            "Iter: 305\n",
            "Iter: 306\n",
            "Iter: 307\n",
            "Iter: 308\n",
            "Iter: 309\n",
            "Iter: 310\n",
            "Iter: 311\n",
            "Iter: 312\n",
            "Iter: 313\n",
            "Iter: 314\n",
            "Iter: 315\n",
            "Iter: 316\n",
            "Iter: 317\n",
            "Iter: 318\n",
            "Iter: 319\n",
            "Iter: 320\n",
            "Iter: 321\n",
            "Iter: 322\n",
            "Iter: 323\n",
            "Iter: 324\n",
            "Iter: 325\n",
            "Iter: 326\n",
            "Iter: 327\n",
            "Iter: 328\n",
            "Iter: 329\n",
            "Iter: 330\n",
            "Iter: 331\n",
            "Iter: 332\n",
            "Iter: 333\n",
            "Iter: 334\n",
            "Iter: 335\n",
            "Iter: 336\n",
            "Iter: 337\n",
            "Iter: 338\n",
            "Iter: 339\n",
            "Iter: 340\n",
            "Iter: 341\n",
            "Iter: 342\n",
            "Iter: 343\n",
            "Iter: 344\n",
            "Iter: 345\n",
            "Iter: 346\n",
            "Iter: 347\n",
            "Iter: 348\n",
            "Iter: 349\n",
            "Iter: 350\n",
            "Iter: 351\n",
            "Iter: 352\n",
            "Iter: 353\n",
            "Iter: 354\n",
            "Iter: 355\n",
            "Iter: 356\n",
            "Iter: 357\n",
            "Iter: 358\n",
            "Iter: 359\n",
            "Iter: 360\n",
            "Iter: 361\n",
            "Iter: 362\n",
            "Iter: 363\n",
            "Iter: 364\n",
            "Iter: 365\n",
            "Iter: 366\n",
            "Iter: 367\n",
            "Iter: 368\n",
            "Iter: 369\n",
            "Iter: 370\n",
            "Iter: 371\n",
            "Iter: 372\n",
            "Iter: 373\n",
            "Iter: 374\n",
            "Iter: 375\n",
            "Iter: 376\n",
            "Iter: 377\n",
            "Iter: 378\n",
            "Iter: 379\n",
            "Iter: 380\n",
            "Iter: 381\n",
            "Iter: 382\n",
            "Iter: 383\n",
            "Iter: 384\n",
            "Iter: 385\n",
            "Iter: 386\n",
            "Iter: 387\n",
            "Iter: 388\n",
            "Iter: 389\n",
            "Iter: 390\n",
            "Iter: 391\n",
            "Iter: 392\n",
            "Iter: 393\n",
            "Iter: 394\n",
            "Iter: 395\n",
            "Iter: 396\n",
            "Iter: 397\n",
            "Iter: 398\n",
            "Iter: 399\n",
            "Iter: 400\n",
            "Iter: 401\n",
            "Iter: 402\n",
            "Iter: 403\n",
            "Iter: 404\n",
            "Iter: 405\n",
            "Iter: 406\n",
            "Iter: 407\n",
            "Iter: 408\n",
            "Iter: 409\n",
            "Iter: 410\n",
            "Iter: 411\n",
            "Iter: 412\n",
            "Iter: 413\n",
            "Iter: 414\n",
            "Iter: 415\n",
            "Iter: 416\n",
            "Iter: 417\n",
            "Iter: 418\n",
            "Iter: 419\n",
            "Iter: 420\n",
            "Iter: 421\n",
            "Iter: 422\n",
            "Iter: 423\n",
            "Iter: 424\n",
            "Iter: 425\n",
            "Iter: 426\n",
            "Iter: 427\n",
            "Iter: 428\n",
            "Iter: 429\n",
            "Iter: 430\n",
            "Iter: 431\n",
            "Iter: 432\n",
            "Iter: 433\n",
            "Iter: 434\n",
            "Iter: 435\n",
            "Iter: 436\n",
            "Iter: 437\n",
            "Iter: 438\n",
            "Iter: 439\n",
            "Iter: 440\n",
            "Iter: 441\n",
            "Iter: 442\n",
            "Iter: 443\n",
            "Iter: 444\n",
            "Iter: 445\n",
            "Iter: 446\n",
            "Iter: 447\n",
            "Iter: 448\n",
            "Iter: 449\n",
            "Iter: 450\n",
            "Iter: 451\n",
            "Iter: 452\n",
            "\n",
            "Epoch: 6\n",
            "Present Rate: 3.125e-07\n",
            "Validation Macro: 0.7948717948717949\n",
            "Backtrack\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVfQCFrAQg-2"
      },
      "source": [
        "import torch\n",
        "m = \n",
        "m = torch.load(torch.load_state_dict(\"Hate-Speech-Detectionadaptive.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJemSU5sVMiV"
      },
      "source": [
        "!pip install ekphrasis\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "seg_tw = Segmenter(corpus = \"twitter\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ710E64VWJq"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "import preprocessor as tweet_proc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjiYpH3YVvtb",
        "outputId": "ea7402dd-8b8d-4865-eedf-6a3b72ee5210"
      },
      "source": [
        "!pip install emot\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as tweet_proc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYeEUwJYV8S2"
      },
      "source": [
        "def make_list(proc_obj):\n",
        "  if proc_obj == None:\n",
        "    return []\n",
        "  \n",
        "  store = []\n",
        "  for unit in proc_obj:\n",
        "    store.append(unit.match)\n",
        "  \n",
        "  return store"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPbso8S-WQOy"
      },
      "source": [
        "def emotext(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNWp7JUpWT56"
      },
      "source": [
        "file_name = \"hate_speech.tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHU4zyZVWZH0",
        "outputId": "dbdc9a6f-f50a-45ff-8154-e52c29179291"
      },
      "source": [
        "german = False\n",
        "\n",
        "datapoints_count = 0\n",
        "see_index = True\n",
        "\n",
        "tweets = []\n",
        "raw_tweet_texts = []\n",
        "\n",
        "hashtags = []\n",
        "smileys = []\n",
        "emojis = []\n",
        "urls = []\n",
        "mentions = []\n",
        "numbers = []\n",
        "reserveds = []\n",
        "\n",
        "task_1_labels = []\n",
        "task_2_labels = []\n",
        "task_3_labels = []\n",
        "\n",
        "with open(file_name) as file:\n",
        "    file_reader = csv.reader(file, delimiter = \"\\t\")\n",
        "    for line in file_reader:\n",
        "      if see_index == True:\n",
        "        see_index = False\n",
        "        continue\n",
        "      \n",
        "      datapoints_count += 1\n",
        "\n",
        "      task_1_labels.append(line[1])\n",
        "\n",
        "     \n",
        "      \n",
        "      tweets.append(line[0])\n",
        "      raw_tweet_texts.append(tweet_proc.clean(line[0]))\n",
        "\n",
        "      parse_obj = tweet_proc.parse(line[0])\n",
        "\n",
        "      hashtags.append(make_list(parse_obj.hashtags))\n",
        "      smileys.append(make_list(parse_obj.smileys))\n",
        "      emojis.append(make_list(parse_obj.emojis))\n",
        "      urls.append(make_list(parse_obj.urls))\n",
        "      mentions.append(make_list(parse_obj.mentions))\n",
        "      numbers.append(make_list(parse_obj.numbers))\n",
        "      reserveds.append(make_list(parse_obj.reserved))\n",
        "\n",
        "print(\"Number of Datapoints: \" + str(datapoints_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Datapoints: 4578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pLqeMNXXQB7",
        "outputId": "ee27637f-04c9-4519-85d2-99886b431e8c"
      },
      "source": [
        "emoji_texts = []\n",
        "\n",
        "for emo_list in emojis:\n",
        "  texts = []\n",
        "  for emoji in emo_list:\n",
        "    print(emoji)\n",
        "    text = emotext(emoji)\n",
        "    texts.append(text)\n",
        "  emoji_texts.append(texts)\n",
        "\n",
        "print(\"Emoji Descriptions:\")\n",
        "print(emoji_texts[0: 5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ðŸ–“\n",
            "â™«\n",
            "Emoji Descriptions:\n",
            "[[], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C83kMnPRXfhD"
      },
      "source": [
        "segmented_hashtags = []\n",
        "\n",
        "for hashset in hashtags:\n",
        "  segmented_set = []\n",
        "  for tag in hashset:\n",
        "    word = tag[1: ]\n",
        "    # removing the hash symbol\n",
        "    segmented_set.append(seg_tw.segment(word))\n",
        "  segmented_hashtags.append(segmented_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tkcmwzKfeEL"
      },
      "source": [
        "name = 'hi-en.pickle'\n",
        "dickie = {}\n",
        "\n",
        "dickie['task_1'] = task_1_labels\n",
        "\n",
        "\n",
        "dickie['full_tweet'] = tweets\n",
        "dickie['tweet_raw_text'] = raw_tweet_texts\n",
        "dickie['hashtags'] = hashtags\n",
        "dickie['smiley'] = smileys\n",
        "dickie['emoji'] = emojis\n",
        "dickie['url'] = urls\n",
        "dickie['mentions'] = mentions\n",
        "dickie['numerals'] = numbers\n",
        "dickie['reserved_word'] = reserveds\n",
        "dickie['emotext'] = emoji_texts\n",
        "dickie['segmented_hash'] = segmented_hashtags\n",
        "\n",
        "with open(name, 'wb') as f:\n",
        "  pickle.dump(dickie, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689,
          "referenced_widgets": [
            "6cf00bc89dea4838a8d61d00fe1f3dee",
            "87454a2bf8784f0f8778588ca495495e",
            "40c70f2b723e41f7a5b1069cf77937d0",
            "38e0d24a8cf74be59a0c66293e98e307",
            "1db1597778d44d8f980e7f4722355702",
            "86d0bdaa039b4ae3ba31800c0e322b10",
            "0559baf2d8844ba39bd2a9f3738b00da",
            "6e60ee394c5d40319f3e1a4f6cecd8c4",
            "adf79b769b984963aaefd62ff6947e02",
            "85078aabe8014d72aafa1398963c880a",
            "aaf5e194359142e6a88c46f830346b64",
            "715de49997b34fc4b32e7e03b75df6a9",
            "59aa2a7926e54df59edfb9a01621b18c",
            "99ab377622ce4811a89563ff8dac69d6",
            "2988257d774f4e888abff75761555545",
            "9075d6edeb644553a598cc68c89310b0"
          ]
        },
        "id": "ew977KS6ZeWH",
        "outputId": "4559d20b-6a73-4662-dbba-04c79f7ee76a"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "bert = XLMRobertaModel.from_pretrained(\"sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/92/bd06be977adfe6cd92038f8c263313961980617890daf3f0de636395a3ef/sacremoses-0.0.45.tar.gz (880kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 880kB 33.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.45-cp37-none-any.whl size=894380 sha256=0b61d70a6ecc71992143a7cdb1abbf1cd329dd7f24e51d613ba34db72c1b4c90\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/25/3e/96b676a0ee4c1ef81bbd8ebb703d5f08bd2b838415fba4a594\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cf00bc89dea4838a8d61d00fe1f3dee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=541.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adf79b769b984963aaefd62ff6947e02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1112256686.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf_awEEBhzEO"
      },
      "source": [
        "import pandas as pd\n",
        "import xlrd\n",
        "import re\n",
        "import pickle\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTqmbI5AIQ67",
        "outputId": "97329569-094b-4243-c212-1aef3bf05600"
      },
      "source": [
        "!pip install ekphrasis\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "seg_tw = Segmenter(corpus = \"twitter\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184kB 27.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp37-none-any.whl size=82844 sha256=1eff12ce63ea0f308ad9bbb53fc8d6f09af7bed6a55f14dc745ec477b4c9fbc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=f3757ddf4bf4bc1a3387f922e5a0717191f63c1613d0fa9c795c3efacfd656a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-6.0.1 ujson-4.0.2\n",
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPcgARX-IYdv",
        "outputId": "3eb9a3d9-8d40-4055-c9f6-ffdbdab62027"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "import preprocessor as tweet_proc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgPIbr8YIuyW",
        "outputId": "c42a89be-fd64-4c26-97ec-faed4fbeaacf"
      },
      "source": [
        "!pip install emot\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOqszo5AIyRR",
        "outputId": "c70fa87b-8067-4d90-e0dd-15d972583293"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQn4R8WbI26J"
      },
      "source": [
        "def make_list(proc_obj):\n",
        "  if proc_obj == None:\n",
        "    return []\n",
        "  \n",
        "  store = []\n",
        "  for unit in proc_obj:\n",
        "    store.append(unit.match)\n",
        "  \n",
        "  return store\n",
        "\n",
        "def emotext(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjZpPQBSJDZ1"
      },
      "source": [
        "is_hindi = 0\n",
        "\n",
        "datatype = \"test\"\n",
        "\n",
        "file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/english_test_1509.csv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivgQkB3bJMoV"
      },
      "source": [
        "datapoints_count = 0\n",
        "see_index = True\n",
        "\n",
        "tweets = []\n",
        "raw_tweet_texts = []\n",
        "tokenized_tweets = []\n",
        "hashtags = []\n",
        "smileys = []\n",
        "emojis = []\n",
        "urls = []\n",
        "mentions = []\n",
        "numbers = []\n",
        "reserveds = []\n",
        "\n",
        "task_1_labels = []\n",
        "task_2_labels = []\n",
        "tweet_ids = []\n",
        "hasoc_ID = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnrS5yqJJPcw"
      },
      "source": [
        "if datatype == 'test':\n",
        "    file = open(file_name, 'r')\n",
        "    file_reader = csv.reader(file, delimiter = \",\")\n",
        "    for line in file_reader:\n",
        "        if see_index == True:\n",
        "            see_index = False\n",
        "            continue\n",
        "\n",
        "        datapoints_count += 1\n",
        "        tweet_ids.append(line[0])\n",
        "        task_1_labels.append(line[2])\n",
        "        task_2_labels.append(line[3])\n",
        "        hasoc_ID.append(line[4])\n",
        "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
        "\n",
        "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
        "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
        "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
        "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
        "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
        "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
        "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
        "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
        "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
        "\n",
        "        if is_hindi == 0:\n",
        "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
        "        else:\n",
        "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
        "\n",
        "    print(\"Number of Datapoints: \" + str(datapoints_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8be7qSTNb6qw",
        "outputId": "c2768457-7969-4d05-83c1-7a15fc4d28cc"
      },
      "source": [
        "!git clone https://github.com/sayarghoshroy/Hate-Speech-Detection.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Hate-Speech-Detection'...\n",
            "remote: Enumerating objects: 280, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 280 (delta 113), reused 189 (delta 59), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (280/280), 13.28 MiB | 15.86 MiB/s, done.\n",
            "Resolving deltas: 100% (113/113), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN5YciVRRu8t"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "data = pickle.load(open(\"/content/Hate-Speech-Detection/data/2020_processed_train/en.pickle\",'rb'))\n",
        "df = pd.DataFrame.from_dict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "id": "NpsLJN4icCyq",
        "outputId": "e6507ffd-e6df-47a6-d24d-8b1c3ee49734"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>hasoc_id</th>\n",
              "      <th>full_tweet</th>\n",
              "      <th>tweet_raw_text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>smiley</th>\n",
              "      <th>emoji</th>\n",
              "      <th>url</th>\n",
              "      <th>mentions</th>\n",
              "      <th>numerals</th>\n",
              "      <th>reserved_word</th>\n",
              "      <th>emotext</th>\n",
              "      <th>segmented_hash</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1123757263427186690</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>hasoc_2020_en_2574</td>\n",
              "      <td>hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...</td>\n",
              "      <td>hate wen females hit ah nigga with tht bro , I...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[ðŸ˜‚, ðŸ˜‚]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[face with tears of joy, face with tears of joy]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1123733301397733380</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>hasoc_2020_en_3627</td>\n",
              "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
              "      <td>: When you're from the Bay but you're really a...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[https://t.co/mZ8BAYlnlf]</td>\n",
              "      <td>[@airjunebug, @supportcaleon]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1123734094108659712</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_3108</td>\n",
              "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
              "      <td>: Dear Democrats: The American people arent st...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[@DonaldJTrumpJr]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1126951188170199049</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>hasoc_2020_en_3986</td>\n",
              "      <td>RT @SheLoveTimothy: He ainâ€™t on drugs he just ...</td>\n",
              "      <td>: He aint on drugs he just bored. I be doing t...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[ðŸ˜‚]</td>\n",
              "      <td>[https://t.co/tkdjSbddET]</td>\n",
              "      <td>[@SheLoveTimothy]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[face with tears of joy]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1126863510447710208</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_5152</td>\n",
              "      <td>RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...</td>\n",
              "      <td>: Summer Im coming for you ! No boring shit ! ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[@TavianJordan]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3703</th>\n",
              "      <td>1126887103437123584</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_109</td>\n",
              "      <td>RT @FilthyArt_: TONIGHT TONIGHT TONIGHT \\n\\nCa...</td>\n",
              "      <td>: TONIGHT TONIGHT TONIGHT Catch me doing some ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[âœ¨]</td>\n",
              "      <td>[https://t.co/AUMpsdJYW8]</td>\n",
              "      <td>[@FilthyArt_]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[sparkles]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3704</th>\n",
              "      <td>1126825614906937344</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>hasoc_2020_en_2345</td>\n",
              "      <td>RT @abbn0rmal_: Eat my ass</td>\n",
              "      <td>: Eat my ass</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[@abbn0rmal_]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705</th>\n",
              "      <td>1126880392550731776</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_1039</td>\n",
              "      <td>RT @FlyTPA: BREAKING NEWS: TPA is about to get...</td>\n",
              "      <td>: BREAKING NEWS: TPA is about to get even bett...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[ðŸ‘©, ðŸ‘¦, ðŸ’§]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[@FlyTPA]</td>\n",
              "      <td>[ 2020]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[woman, boy, droplet]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3706</th>\n",
              "      <td>1130290906932891648</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>hasoc_2020_en_2817</td>\n",
              "      <td>RT @StarrThaRapper: Itâ€™s been a hr FUCK THAT G...</td>\n",
              "      <td>: Its been a hr FUCK THAT GAME</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[ðŸ‘¿, ðŸ‘¿]</td>\n",
              "      <td>[https://t.co/AkI6BW8Qlz]</td>\n",
              "      <td>[@StarrThaRapper]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[angry face with horns, angry face with horns]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3707</th>\n",
              "      <td>1126979503933149184</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_1585</td>\n",
              "      <td>You don't know who iam but i know youðŸ˜„</td>\n",
              "      <td>You don't know who iam but i know you</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[ðŸ˜„]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[smiling face with open mouth &amp; smiling eyes]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3708 rows Ã— 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 tweet_id  ... segmented_hash\n",
              "0     1123757263427186690  ...             []\n",
              "1     1123733301397733380  ...             []\n",
              "2     1123734094108659712  ...             []\n",
              "3     1126951188170199049  ...             []\n",
              "4     1126863510447710208  ...             []\n",
              "...                   ...  ...            ...\n",
              "3703  1126887103437123584  ...             []\n",
              "3704  1126825614906937344  ...             []\n",
              "3705  1126880392550731776  ...             []\n",
              "3706  1130290906932891648  ...             []\n",
              "3707  1126979503933149184  ...             []\n",
              "\n",
              "[3708 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2OYf3EecDLl"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install bert-tensorflow\n",
        "!pip install transformers\n",
        "!pip install seaborn\n",
        "!pip install sklearn-crfsuite\n",
        "!pip install -U sentence-transformers\n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvAA9cxhi0co"
      },
      "source": [
        "import random\n",
        "import pickle\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "# Check where we need this\n",
        "# from nltk.corpus import stopwordsm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "import gensim.models as gsm\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "from tqdm import tqdm \n",
        "import gc\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oByUlBKi4u1",
        "outputId": "3f166107-bd0b-47a6-d304-59ef639aa268"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV8aQVkVjL8C"
      },
      "source": [
        "data_loc = '/content/Hate-Speech-Detection/data/2020_processed_train/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dRHg3S8korm"
      },
      "source": [
        "e2v = gsm.KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
        "\n",
        "def getEmojiEmbeddings(emojiList,dim=300,verbose = False):\n",
        "  # Generates an emoji vector by averaging the emoji representation for each emoji\n",
        "  # If no emoji returns an empty list of dimension dim\n",
        "  if dim < 300:\n",
        "    raise IndexError(\"Dim has to be greater than 300\")\n",
        "  result = np.zeros(dim)\n",
        "  if (len(emojiList) == 0):\n",
        "    return result\n",
        "  else:\n",
        "    embs = None\n",
        "    for i in emojiList:\n",
        "      if verbose:\n",
        "        if i not in e2v.vocab:\n",
        "          print(i)\n",
        "    embs = np.mean([e2v[i] for i in emojiList if i in e2v.vocab], axis=0)\n",
        "  if np.any(np.isnan(embs)):\n",
        "    return result\n",
        "  result[:300] = embs\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o1UF1sEpW3I"
      },
      "source": [
        "def loadData(lang):\n",
        "  \"\"\" Function to load data for one language from the preprocessed pickle file\"\"\"\n",
        "  if lang not in ['hi','en','hi-en']:\n",
        "      raise NameError(\"Language not found\")\n",
        "  fileName = lang + '.pickle'\n",
        "  with open(DATASET_ROOT+fileName, 'rb') as f:\n",
        "    ged = pickle.load(f)\n",
        "  df = pd.DataFrame.from_dict(ged)\n",
        "  if lang in ['hi','en']:\n",
        "    df = df.drop(['tweet_id','task_2','hasoc_id'],axis=1)\n",
        "  if lang == 'hi-en':\n",
        "    df.task_1[df.task_1.str.startswith(\"y\")] = \"HOF\"\n",
        "    df.task_1[df.task_1.str.startswith(\"n\")] = \"NOT\"\n",
        "    df.task_1[df.task_1.str.startswith(\"o\")] = \"HOF\"\n",
        "  train_df, test_df = model_selection.train_test_split(df, random_state = 42, test_size = 0.25)\n",
        "  return train_df, test_df, df\n",
        "\n",
        "def loadDataAllLangs():\n",
        "  \"\"\" Function to load data for all languages from the preprocessed pickle file\"\"\"\n",
        "\n",
        "  hi_train,hi_test,hi_df = loadData('hi')\n",
        "  en_train,en_test,en_df = loadData('en')\n",
        "  ge_train,ge_test,ge_df = loadData('hi-en')\n",
        "  print(\"total size:\", len(ge_df) + len(hi_df)+len(en_df))\n",
        "  train_df = pd.concat([hi_train,en_train,ge_train],ignore_index=True)\n",
        "  test_df =  pd.concat([hi_test,en_test,ge_test],ignore_index=True)\n",
        "  df = pd.concat([hi_df,en_df,ge_df],ignore_index=True)\n",
        "  train_df = train_df.sample(frac = 1, random_state=42)\n",
        "  test_df = test_df.sample(frac = 1, random_state=42)\n",
        "  df = df.sample(frac = 1, random_state=42)\n",
        "  return train_df,test_df,df\n",
        "\n",
        "class HASOCDataset(Dataset):\n",
        "  \"\"\" Data loader to load the data for the Torch \"\"\"\n",
        "  def __init__(self, dataPath, isDF = False):\n",
        "    if isDF:\n",
        "      self.df = pd.DataFrame.from_dict(dataPath)\n",
        "    else:\n",
        "      data = pickle.load(open(dataPath,'rb'))\n",
        "      self.df = pd.DataFrame.from_dict(data)\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  def __getitem__(self,index):\n",
        "    return self.df.iloc[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coz7nYQQpffE"
      },
      "source": [
        "def set_seed(seed):\n",
        "     # \"\"\" Sets all seed to the given value, so we can reproduce (:3) \"\"\"\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KglXJWpbrXsI"
      },
      "source": [
        "class FullExample(object):\n",
        "  \"\"\" Not necessary any more, mainly here in case we might need to use the entire thing. \"\"\"\n",
        "  def __init__(self, task_1, full_tweet, tweet_raw_text, hashtags, smiley, emoji, url, mentions, numerals, reserved_word, segmented_hash):\n",
        "    self.task_1 = task_1\n",
        "    self.full_tweet = full_tweet\n",
        "    self.tweet_raw_text = tweet_raw_text\n",
        "    self.hashtags = hashtags\n",
        "    self.smiley = smiley\n",
        "    self.emoji = emoji\n",
        "    self.url = url \n",
        "    self. mentions = mentions \n",
        "    self.numerals = numerals\n",
        "    self.reserved_word = reserved_word\n",
        "    self.segmented_hash = segmented_hash\n",
        "  \n",
        "class Example(object):\n",
        "  \"\"\" Contains the data for one example from the dataset \"\"\"\n",
        "  def __init__(self,id, task_1, full_tweet, tweet_raw_text,  emoji,  segmented_hash):\n",
        "    self.id  = id\n",
        "    self.task_1 = task_1\n",
        "\n",
        "    self.full_tweet = full_tweet\n",
        "    self.tweet_raw_text = tweet_raw_text\n",
        "    self.emoji = emoji\n",
        "    self.segmented_hash = segmented_hash\n",
        "\n",
        "class ExampleFeautres(object):\n",
        "    \"\"\" Contains the dataset in a batch friendly feaute set \"\"\"\n",
        "    def __init__(self, id, task_1, input_ids, input_mask,input_length,  emoji,  hash):\n",
        "      self.id  = id\n",
        "      self.task_1 = task_1\n",
        "      self.emoji = torch.tensor(emoji)\n",
        "      self.input_ids = input_ids\n",
        "      self.input_mask = input_mask\n",
        "      self.input_length = input_length \n",
        "      self.hash = torch.tensor(hash)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "30a25c1567404738ae1ea967183f2f41",
            "0016a2fab7e14c959b1cda0108c2916a",
            "efa4cfb566ea4948a403a78e712869d3",
            "f94112492bf24936a23597050761f462",
            "1885716c90394985b6f86ccb61510ecf",
            "773d793fef6a46778fed40536c37a64b",
            "602d321380ee4128b1bf01382b030a28",
            "0311e2ea8aa548c58f0be83838b74fc8",
            "da12028fd51441ef854434255bd63690",
            "d1cebf94428a47b08d91eb3d376cb864",
            "fe877a8afec641df82e69caecd3faad9",
            "0725ac84c0fe4db4a5b8ee5e5f15788b",
            "a2842421fa244fcd9385ccff1466996d",
            "b26de9c295ed4a4b893bd2d799793e7e",
            "844af92f3dd8438aac91ac8c76253909",
            "5813c1af2aba47359a640a34c10ba246",
            "ed07bff31bde4d148b8d8bcf4734b9bb",
            "479c5e05879a4821ae1b5b1f00033819",
            "14e98f59bdf447ccb4c8f2dca5464e86",
            "942d34c4a6ab4e509ae903db2d7857e9",
            "ca2dac121a114882b20828bd63e956f5",
            "9f0484de057d4a52bb9c15d6a52263fc",
            "c008421054a54fa5a4733df127dbf34b",
            "9ee0666b9c41422f94e16f09b2b2d729",
            "d7ce9b96da854bb9893ec8fa1cf6033e",
            "d23d373f144247bb9e0f06050f1869d3",
            "317c1e42df3541cfacfabd601995fe9f",
            "b32926a9b7d14e048056751b4550737f",
            "d9c3cdd6706841d78fb508ccb6e58376",
            "d90fa9dc19cc4f1c84adb8cc2560b6fa",
            "e01d7e6e934c42e8bab30a67cad61617",
            "64d84831efd74489b7493a34bafd62fc"
          ]
        },
        "id": "ck6UNYRFrxpY",
        "outputId": "fef987da-3a4c-4025-c2eb-8e74808073f6"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens\")\n",
        "max_seq_length = 74\n",
        "# e2v = gsm.KeyedVectors.load_word2vec_format('/content/drive/My Drive/emoji2vec.bin', binary = True)\n",
        "sent_encoder = SentenceTransformer('xlm-r-100langs-bert-base-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30a25c1567404738ae1ea967183f2f41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da12028fd51441ef854434255bd63690",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=150.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed07bff31bde4d148b8d8bcf4734b9bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=147.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7ce9b96da854bb9893ec8fa1cf6033e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1014108634.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeE_mfYxr4xk"
      },
      "source": [
        "labels_task1 = {'NOT':0, 'HOF':1}\n",
        "labels_task2 = {'NONE':0,'PRFN':1,'OFFN':2,'HATE':3}\n",
        "\n",
        "def convertExamplesToFeature(example):\n",
        "  \"\"\" Given a data row convert it to feautres so it's batch friendly \"\"\"\n",
        "  raw_text = example.tweet_raw_text\n",
        "  tokens = tokenizer.tokenize(raw_text)\n",
        "  if (len(tokens) > (max_seq_length-2)):\n",
        "    tokens = tokens[: (max_seq_length-2)]\n",
        "  tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  input_mask = [1] * len(input_ids)\n",
        "  input_length = len(input_ids)\n",
        "  padding = [0] * (max_seq_length - len(input_ids))\n",
        "  input_ids += padding\n",
        "  input_mask += padding\n",
        "  hashtags = ' '.join(example.segmented_hash)\n",
        "  hashembs = sent_encoder.encode(hashtags)\n",
        "\n",
        "  emojiVec = getEmojiEmbeddings(example.emoji)\n",
        "  task1 = labels_task1[example.task_1]\n",
        "  id = example.id\n",
        "\n",
        "  return ExampleFeautres(id,task1, input_ids, input_mask, input_length, emojiVec, hashembs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js2kmoUIsMJO"
      },
      "source": [
        "def getDataset(input_features):\n",
        "    \"\"\"\n",
        "    Mappings for index-> features \n",
        "    0 -> ID\n",
        "    1 -> input ids\n",
        "    2 -> input masks\n",
        "    3 -> input lengths \n",
        "    4 -> hash embs \n",
        "    5 -> emoji embs \n",
        "    6 -> task1\n",
        "    7 -> task2\n",
        "    \"\"\"\n",
        "    all_input_page_ids = torch.tensor([f.id for f in input_features], dtype=torch.long)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in input_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in input_features], dtype=torch.long)\n",
        "    all_input_lengths = torch.tensor([f.input_length for f in input_features], dtype=torch.long)\n",
        "    all_hash_embs = torch.stack([f.hash for f in input_features])\n",
        "    all_emoji_embs = torch.stack([f.emoji for f in input_features])\n",
        "    all_task_1 = torch.tensor([f.task_1 for f in input_features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_page_ids, all_input_ids, all_input_mask,all_input_lengths, all_hash_embs, all_emoji_embs, all_task_1)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1waujKTseJa"
      },
      "source": [
        "def train_val_dataset(dataset, val_split = 0.2):\n",
        "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
        "    datasets = {}\n",
        "    datasets['train'] = Subset(dataset, train_idx)\n",
        "    datasets['valid'] = Subset(dataset, val_idx)\n",
        "    return datasets\n",
        "\n",
        "def getDataloader(path_to_pickle, val_split = 0.2, batch_size = 16, multiLing = True):\n",
        "  if multiLing:\n",
        "    tr,tt,df = loadDataAllLangs()\n",
        "    tempDataset = HASOCDataset(df, isDF=True)\n",
        "  else:\n",
        "    print(path_to_pickle)\n",
        "    tempDataset = HASOCDataset(path_to_pickle)\n",
        "\n",
        "  input_features = []\n",
        "  for i in tqdm(range(len(tempDataset))):\n",
        "    example = Example(i,tempDataset[i]['task_1'], tempDataset[i]['full_tweet'],tempDataset[i]['tweet_raw_text'], tempDataset[i]['emoji'],tempDataset[i]['segmented_hash'])\n",
        "    input_feature = convertExamplesToFeature(example)\n",
        "    input_features.append(input_feature)\n",
        "  dataset = getDataset(input_features)\n",
        "  # print(len(dataset))\n",
        "  set_seed(42)\n",
        "  data_sampler = RandomSampler(dataset)\n",
        "  dd = train_val_dataset(dataset, val_split)\n",
        "  train_dataloader = DataLoader(dd['train'], sampler = RandomSampler(dd['train']), batch_size=batch_size, drop_last=True)\n",
        "  valid_dataloader = DataLoader(dd['valid'] , batch_size=batch_size, drop_last=True)\n",
        "  dataloader = DataLoader(dataset , batch_size=batch_size, drop_last=True)\n",
        "  dataloaders = {x:DataLoader(dd[x], 32, shuffle = True, num_workers = 4) for x in ['train','valid']} \n",
        "\n",
        "  return train_dataloader, valid_dataloader, dataloader, dataloaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc92F_PUsiG2"
      },
      "source": [
        "DATASET_ROOT = data_loc\n",
        "train_dataloader, valid_dataloader, dataloader, dataloaders = getDataloader(data_loc , multiLing = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XmXPil74Zjj"
      },
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "  \"\"\" Classification head for the Roberta Model \"\"\" \n",
        "  def __init__(self, numberOfClasses, hidden_size_bert, hidden_size_post_feats, dropout_val = 0.2):\n",
        "    super().__init__()\n",
        "    self.denseInit = nn.Linear(hidden_size_post_feats, hidden_size_bert)\n",
        "    self.dense = nn.Linear(hidden_size_bert, hidden_size_bert)\n",
        "    self.dropout = nn.Dropout(dropout_val)\n",
        "    self.output = nn.Linear(hidden_size_bert, numberOfClasses)\n",
        "  def forward(self, x):\n",
        "    # print(x.shape)\n",
        "    x = self.dropout(x)\n",
        "    x = self.denseInit(x)\n",
        "    x = torch.tanh(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense(x)\n",
        "    x  = torch.tanh(x)\n",
        "    x = self.dropout(x)\n",
        "    x  = self.output(x)\n",
        "    return x\n",
        "\n",
        "class TextClassification(nn.Module):\n",
        "  \"\"\" Classifier with feature injection \"\"\"\n",
        "  def __init__(self, numberOfClasses,dropout_val = 0.1, batch_size = 16):\n",
        "     super(TextClassification, self).__init__()\n",
        "     self.bert = XLMRobertaModel.from_pretrained(\"sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens\")\n",
        "     self.classifier = ClassificationHead(numberOfClasses, self.bert.config.hidden_size, (self.bert.config.hidden_size * 2 + 300) , dropout_val)\n",
        "  def forward(self, input_seq, attention_mask, emoji, hashTag):\n",
        "    bert_pooled_output = self.bert(input_seq, attention_mask=attention_mask)[0]\n",
        "    bert_pooled_output = bert_pooled_output[:, 0, :]\n",
        "    bert_pooled_out_feat = torch.cat([bert_pooled_output, emoji, hashTag], axis = 1)\n",
        "    # print(\"Shape\",bert_pooled_out_feat.shape)\n",
        "    output = self.classifier(bert_pooled_out_feat)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ4sY7kV6jhz"
      },
      "source": [
        "model_name = 'adaptive'\n",
        "model_loc = 'hasoc_saved/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVjGh3nj6oan"
      },
      "source": [
        "def modelEvaluate(model, valid_dataloader = valid_dataloader, task = 1):\n",
        "  gc.collect()\n",
        "  if task == 1:\n",
        "    taskIndex = 6\n",
        "  elif task == 2:\n",
        "    taskIndex = 7\n",
        "  model.eval()\n",
        "  predictions, true_labels = [], []\n",
        "  logits = []\n",
        "  # Predict \n",
        "  for batch in valid_dataloader:\n",
        "    # Add batch to GPU\n",
        "    b_input_ids = batch[1]\n",
        "    b_input_mask = batch[2]\n",
        "    b_labels = batch[taskIndex]\n",
        "    b_emoji = batch[5]\n",
        "    b_hashtag = batch[4]\n",
        "    with torch.no_grad():\n",
        "      pred = model(b_input_ids,b_input_mask ,b_emoji.float(), b_hashtag.float())\n",
        "    logits.append(pred.detach().cpu().numpy())\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    flat_true_labels = np.concatenate(true_labels, axis = 0)\n",
        "    predictions = []\n",
        "    for i in logits:\n",
        "      for j in i:\n",
        "        predictions.append(j)\n",
        "    flat_predictions = [np.argmax(i) for i in predictions]\n",
        "    assert(len(flat_predictions) == len(flat_true_labels))\n",
        "    return flat_predictions, flat_true_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4yfldKz6rXg"
      },
      "source": [
        "path =  model_name + \".pt\"\n",
        "scale = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRIkraGw6vf3"
      },
      "source": [
        "def make_optim(model, rate = 2e-5):\n",
        "  return AdamW(model.parameters(),\n",
        "                lr = rate, # default = 5e-5, using 2e-5\n",
        "                eps = 1e-8) # default = 1e-8\n",
        "\n",
        "def train_model(train_dataloader, valid_dataloader, numberOfEpochs = 10, task = 1):\n",
        "  \"\"\" Train Loop for the model \"\"\"\n",
        "  scale = 1\n",
        "  if task == 2:\n",
        "    classNum = 4\n",
        "    taskIndex = 7\n",
        "  elif task == 1:\n",
        "    classNum = 2\n",
        "    taskIndex = 6\n",
        "  else:\n",
        "    raise NameError(\"Task not defined\")\n",
        "  total_steps = len(train_dataloader)\n",
        "  print(\"Start\")\n",
        "\n",
        "  model = TextClassification(classNum) # task 1 \n",
        "  if device == \"gpu\":\n",
        "    model.cuda()\n",
        "  \n",
        "  loss_function = nn.CrossEntropyLoss().to(device)\n",
        "  epoch_loss = 0\n",
        "  batch_accuracy_scores = []\n",
        "  global_pred = []\n",
        "  global_label = []\n",
        "\n",
        "  present_rate = 2e-5\n",
        "  old_best = -1\n",
        "  epoch = 0\n",
        "\n",
        "  while(1):\n",
        "    # when the learn rate falls below a lower threshold, you stop your training\n",
        "    # until that moment, march on\n",
        "    epoch += 1\n",
        "    print(\"\\nEpoch:\", epoch)\n",
        "    print(\"Present Rate: \" + str(present_rate))\n",
        "    optimizer = make_optim(model, present_rate)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "    gc.collect()\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batch_accuracy_scores = []\n",
        "    train_data_count = float(len(train_dataloader))\n",
        "\n",
        "    # to check if performance with default weights\n",
        "    predictions, true_labels = modelEvaluate(model, valid_dataloader, task)\n",
        "    score_now = f1_score(true_labels, predictions, average = 'macro')\n",
        "    print(\"Validation Macro: \" + str(score_now))\n",
        "\n",
        "    if (score_now > old_best):\n",
        "      print(\"Continuing on track\")\n",
        "      old_best = score_now\n",
        "\n",
        "      # delete previous best \n",
        "      delete_filename = path\n",
        "      open(delete_filename, 'w').close() # overwrite and make the file blank instead\n",
        "      os.remove(delete_filename) # delete the blank file from google drive will move the file to bin instead\n",
        "      torch.save(model.state_dict(), path)\n",
        "\n",
        "    else:\n",
        "      print(\"Backtrack\")\n",
        "      model.load_state_dict(torch.load(path))\n",
        "      present_rate /= (4 * scale)\n",
        "      scale *= 4\n",
        "      if present_rate < 1e-8:\n",
        "        break\n",
        "\n",
        "    # For quick eval\n",
        "    cnt = 0\n",
        "    # for i, batch in tqdm(enumerate(train_dataloader)):\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        print(\"Iter: \" + str(cnt + 1))\n",
        "        # COMMENT OUT THE NEXT 2 LINES IN ACTUAL TRAINING\n",
        "        # if cnt == 4:\n",
        "        #   break\n",
        "        cnt += 1\n",
        "        b_input_ids = batch[1]\n",
        "        b_input_mask = batch[2]\n",
        "        b_labels = batch[taskIndex]\n",
        "        b_emoji = batch[5]\n",
        "        b_hashtag = batch[4]\n",
        "        pred = model(b_input_ids,b_input_mask ,b_emoji.float(), b_hashtag.float())\n",
        "        loss = loss_function(pred.view(-1, classNum), b_labels.view(-1))\n",
        "        with torch.no_grad():\n",
        "          epoch_loss += (loss.item() * len(b_labels))\n",
        "          global_pred.append(pred)\n",
        "          global_label.append(b_labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSoyhb8q62xn"
      },
      "source": [
        "gc.collect()\n",
        "model = train_model(train_dataloader, valid_dataloader, 2, task = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "jZTIbsyB66o3",
        "outputId": "47e95519-d34a-4b36-d64b-956a60727a09"
      },
      "source": [
        "for batch in valid_dataloader:\n",
        "    # Add batch to GPU\n",
        "    b_input_ids = batch[1]\n",
        "    b_input_mask = batch[2]\n",
        "    print(batch[taskIndex])\n",
        "    b_labels = batch[taskIndex]\n",
        "    b_emoji = batch[5]\n",
        "    b_hashtag = batch[4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-ec325fb7e832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mb_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mb_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtaskIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtaskIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mb_emoji\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'taskIndex' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK1VFU9X7dEx"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPZe6qWqxf9T"
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora, models, similarities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tURSyRY1xqor",
        "outputId": "f09a0d99-7798-4338-b3d3-1bea01a48b72"
      },
      "source": [
        "!git clone https://github.com/ashishgupta1350/Hindi-English-Code-Mixed-Stemmer.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Hindi-English-Code-Mixed-Stemmer'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 22 (delta 7), reused 9 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVPMLI5QxkYF"
      },
      "source": [
        "w2vModel = gensim.models.Word2Vec.load(\"/content/Hindi-English-Code-Mixed-Stemmer/w2vModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7luYWZ9x00U"
      },
      "source": [
        "similarWordsList =[w2vModel.wv.most_similar(\"kyaa\", topn = 10 )[i][0] for i in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9sh7N0YyHKM",
        "outputId": "4bd298fd-e448-46e4-d4a1-2ec6d69675ee"
      },
      "source": [
        "similarWordsList"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fanaaa',\n",
              " 'sakte',\n",
              " 'maloom',\n",
              " 'alaykum',\n",
              " 'as-salamu',\n",
              " 'aapka',\n",
              " 'behtar',\n",
              " 'lalala',\n",
              " 'intezar',\n",
              " 'bewakoofian']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p58HXkXwyROM",
        "outputId": "21ff8b07-b00b-4e84-d711-33de2e321318"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "   \n",
        "ps = PorterStemmer()\n",
        "  \n",
        "# choose some words to be stemmed\n",
        "words = [\"program\", \"programs\", \"prooogramer\", \"programing\", \"programers\"]\n",
        "  \n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "prooogramer  :  prooogram\n",
            "programing  :  program\n",
            "programers  :  program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf-qOPqZz2k4"
      },
      "source": [
        "import pandas as pd\n",
        "import xlrd\n",
        "import re\n",
        "import pickle\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a4ZHt5R0-FRS",
        "outputId": "4af137f4-5089-4db9-eef8-ce917286b964"
      },
      "source": [
        "!pip install ekphrasis\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "# to leverage word statistics from Twitter\n",
        "seg_tw = Segmenter(corpus = \"twitter\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184kB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp37-none-any.whl size=82844 sha256=b3e7de69d394efc440def236b2613dc0c071ae07fdf983d09a998019e3c2f606\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=880c086b0fae470a91e1234db13232a2ad6bd4ee0c097b76ea18b82c84bed0ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-6.0.1 ujson-4.0.2\n",
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLVbrQXK-Low"
      },
      "source": [
        "import torch.nn as nn\n",
        "class GatedCNN(nn.Module):\n",
        "    '''\n",
        "        In : (N, sentence_len)\n",
        "        Out: (N, sentence_len, embd_size)\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 seq_len,\n",
        "                 vocab_size,\n",
        "                 embd_size,\n",
        "                 n_layers,\n",
        "                 kernel,\n",
        "                 out_chs,\n",
        "                 res_block_count,\n",
        "                 ans_size):\n",
        "        super(GatedCNN, self).__init__()\n",
        "        self.res_block_count = res_block_count\n",
        "        # self.embd_size = embd_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embd_size)\n",
        "\n",
        "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...\n",
        "        self.conv_0 = nn.Conv2d(1, out_chs, kernel, padding=(2, 0))\n",
        "        self.b_0 = nn.Parameter(torch.randn(1, out_chs, 1, 1))\n",
        "        self.conv_gate_0 = nn.Conv2d(1, out_chs, kernel, padding=(2, 0))\n",
        "        self.c_0 = nn.Parameter(torch.randn(1, out_chs, 1, 1))\n",
        "\n",
        "        self.conv = nn.ModuleList([nn.Conv2d(out_chs, out_chs, (kernel[0], 1), padding=(2, 0)) for _ in range(n_layers)])\n",
        "        self.conv_gate = nn.ModuleList([nn.Conv2d(out_chs, out_chs, (kernel[0], 1), padding=(2, 0)) for _ in range(n_layers)])\n",
        "        self.b = nn.ParameterList([nn.Parameter(torch.randn(1, out_chs, 1, 1)) for _ in range(n_layers)])\n",
        "        self.c = nn.ParameterList([nn.Parameter(torch.randn(1, out_chs, 1, 1)) for _ in range(n_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(out_chs*seq_len, ans_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, seq_len)\n",
        "\n",
        "        # Embedding\n",
        "        bs = x.size(0) # batch size\n",
        "        seq_len = x.size(1)\n",
        "        x = self.embedding(x) # (bs, seq_len, embd_size)\n",
        "\n",
        "        # CNN\n",
        "        x = x.unsqueeze(1) # (bs, Cin, seq_len, embd_size), insert Channnel-In dim\n",
        "        # Conv2d\n",
        "        #    Input : (bs, Cin,  Hin,  Win )\n",
        "        #    Output: (bs, Cout, Hout, Wout)\n",
        "        A = self.conv_0(x)      # (bs, Cout, seq_len, 1)\n",
        "        A += self.b_0.repeat(1, 1, seq_len, 1)\n",
        "        B = self.conv_gate_0(x) # (bs, Cout, seq_len, 1)\n",
        "        B += self.c_0.repeat(1, 1, seq_len, 1)\n",
        "        h = A * F.sigmoid(B)    # (bs, Cout, seq_len, 1)\n",
        "        res_input = h # TODO this is h1 not h0\n",
        "\n",
        "        for i, (conv, conv_gate) in enumerate(zip(self.conv, self.conv_gate)):\n",
        "            A = conv(h) + self.b[i].repeat(1, 1, seq_len, 1)\n",
        "            B = conv_gate(h) + self.c[i].repeat(1, 1, seq_len, 1)\n",
        "            h = A * F.sigmoid(B) # (bs, Cout, seq_len, 1)\n",
        "            if i % self.res_block_count == 0: # size of each residual block\n",
        "                h += res_input\n",
        "                res_input = h\n",
        "\n",
        "        h = h.view(bs, -1) # (bs, Cout*seq_len)\n",
        "        out = self.fc(h) # (bs, ans_size)\n",
        "        out = F.log_softmax(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYqp2J2neVhJ"
      },
      "source": [
        "vocab_size      = 2000\n",
        "seq_len         = 21\n",
        "embd_size       = 200\n",
        "n_layers        = 10\n",
        "kernel          = (5, embd_size)\n",
        "out_chs         = 64\n",
        "res_block_count = 5\n",
        "batch_size      = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6VkVgq9exHd"
      },
      "source": [
        "import torch\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embd_size)\n",
        "\n",
        "# nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...\n",
        "conv_0 = nn.Conv2d(1, out_chs, kernel, padding=(2, 0))\n",
        "b_0 = nn.Parameter(torch.randn(1, out_chs, 1, 1))\n",
        "conv_gate_0 = nn.Conv2d(1, out_chs, kernel, padding=(2, 0))\n",
        "c_0 = nn.Parameter(torch.randn(1, out_chs, 1, 1))\n",
        "\n",
        "conv = nn.ModuleList([nn.Conv2d(out_chs, out_chs, (kernel[0], 1), padding=(2, 0)) for _ in range(n_layers)])\n",
        "conv_gate = nn.ModuleList([nn.Conv2d(out_chs, out_chs, (kernel[0], 1), padding=(2, 0)) for _ in range(n_layers)])\n",
        "b = nn.ParameterList([nn.Parameter(torch.randn(1, out_chs, 1, 1)) for _ in range(n_layers)])\n",
        "c = nn.ParameterList([nn.Parameter(torch.randn(1, out_chs, 1, 1)) for _ in range(n_layers)])\n",
        "\n",
        "fc = nn.Linear(out_chs*seq_len, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNWfMtdgffNv",
        "outputId": "bb202672-4110-4f72-c80b-eda8615843f9"
      },
      "source": [
        "bs = x.size(0) # batch size\n",
        "seq_len = x.size(1)\n",
        "x = self.embedding(x) # (bs, seq_len, embd_size)\n",
        "\n",
        "# CNN\n",
        "x = x.unsqueeze(1) # (bs, Cin, seq_len, embd_size), insert Channnel-In dim\n",
        "# Conv2d\n",
        "#    Input : (bs, Cin,  Hin,  Win )\n",
        "#    Output: (bs, Cout, Hout, Wout)\n",
        "A = self.conv_0(x)      # (bs, Cout, seq_len, 1)\n",
        "A += self.b_0.repeat(1, 1, seq_len, 1)\n",
        "B = self.conv_gate_0(x) # (bs, Cout, seq_len, 1)\n",
        "B += self.c_0.repeat(1, 1, seq_len, 1)\n",
        "h = A * F.sigmoid(B)    # (bs, Cout, seq_len, 1)\n",
        "res_input = h # TODO this is h1 not h0\n",
        "\n",
        "for i, (conv, conv_gate) in enumerate(zip(self.conv, self.conv_gate)):\n",
        "    A = conv(h) + self.b[i].repeat(1, 1, seq_len, 1)\n",
        "    B = conv_gate(h) + self.c[i].repeat(1, 1, seq_len, 1)\n",
        "    h = A * F.sigmoid(B) # (bs, Cout, seq_len, 1)\n",
        "    if i % self.res_block_count == 0: # size of each residual block\n",
        "        h += res_input\n",
        "        res_input = h\n",
        "\n",
        "h = h.view(bs, -1) # (bs, Cout*seq_len)\n",
        "out = self.fc(h) # (bs, ans_size)\n",
        "out = F.log_softmax(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1), padding=(2, 0))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNsnm7_KgAso",
        "outputId": "c106701c-f6af-489d-ad26-b40b378c6704"
      },
      "source": [
        "b_0.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUqap3a5gFPK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}